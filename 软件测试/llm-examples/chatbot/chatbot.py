import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from datetime import datetime
import os

# å®šä¹‰å¯ç”¨æ¨¡å‹åˆ—è¡¨
AVAILABLE_MODELS = {
    "TinyLlama-1.1B-Chat": {
        "path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "description": "è½»é‡çº§èŠå¤©æ¨¡å‹ï¼Œ1.1Bå‚æ•°"
    },
    "OpenChat-3.5": {
        "path": "openchat/openchat_3.5",
        "description": "åŸºäºMistralçš„å¼€æºèŠå¤©æ¨¡å‹"
    },
    "Phi-2": {
        "path": "microsoft/phi-2",
        "description": "å¾®è½¯å¼€å‘çš„2.7Bå‚æ•°æ¨¡å‹"
    },
    "Qwen-1.5-0.5B": {
        "path": "Qwen/Qwen-1.5-0.5B-Chat",
        "description": "é˜¿é‡Œé€šä¹‰åƒé—®0.5BèŠå¤©æ¨¡å‹"
    }
}

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æœ¬åœ°èŠå¤©æœºå™¨äºº",
    layout="wide"
)

# åˆå§‹åŒ–session state
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "I am an AI assistant, responsible for answering user questions and strictly following user requirements."}
    ]

if "current_chat" not in st.session_state:
    st.session_state["current_chat"] = None

if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = {}

if "current_model" not in st.session_state:
    st.session_state["current_model"] = "TinyLlama-1.1B-Chat"

if "system_prompt" not in st.session_state:
    st.session_state["system_prompt"] = "I am an AI assistant, responsible for answering user questions and strictly following user requirements."

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
if "max_tokens" not in st.session_state:
    st.session_state["max_tokens"] = 256

# åˆ›å»ºä¿å­˜èŠå¤©è®°å½•çš„ç›®å½•
CHAT_HISTORY_DIR = "chat_history"
os.makedirs(CHAT_HISTORY_DIR, exist_ok=True)

def save_chat_history(chat_id, messages):
    """ä¿å­˜èŠå¤©è®°å½•åˆ°æ–‡ä»¶"""
    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)

def load_chat_history(chat_id):
    """ä»æ–‡ä»¶åŠ è½½èŠå¤©è®°å½•"""
    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def get_chat_list():
    """è·å–æ‰€æœ‰èŠå¤©è®°å½•æ–‡ä»¶åˆ—è¡¨"""
    chats = []
    for file in os.listdir(CHAT_HISTORY_DIR):
        if file.endswith(".json"):
            chat_id = file[:-5]  # ç§»é™¤.jsonåç¼€
            file_path = os.path.join(CHAT_HISTORY_DIR, file)
            timestamp = os.path.getmtime(file_path)
            chats.append({
                "id": chat_id,
                "timestamp": timestamp,
                "date": datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
            })
    return sorted(chats, key=lambda x: x["timestamp"], reverse=True)

# æ ‡é¢˜
st.title("ğŸ’¬ æœ¬åœ°æ¨¡å‹èŠå¤©æœºå™¨äºº")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("æ¨¡å‹é€‰æ‹©")
    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
    selected_model = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        options=list(AVAILABLE_MODELS.keys()),
        index=list(AVAILABLE_MODELS.keys()).index(st.session_state["current_model"]),
        help="é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"
    )
    
    # æ˜¾ç¤ºé€‰ä¸­æ¨¡å‹çš„æè¿°
    st.info(AVAILABLE_MODELS[selected_model]["description"])
    
    # å¦‚æœæ¨¡å‹æ”¹å˜ï¼Œæ›´æ–°session state
    if selected_model != st.session_state["current_model"]:
        st.session_state["current_model"] = selected_model
        st.rerun()
    
    st.header("ç³»ç»Ÿæç¤ºè¯")
    # ç³»ç»Ÿæç¤ºè¯è¾“å…¥æ¡†
    system_prompt = st.text_area(
        "è®¾ç½®AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸º",
        value=st.session_state["system_prompt"],
        help="è®¾ç½®AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸ºæ–¹å¼ã€‚ä¾‹å¦‚ï¼š'æˆ‘æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿è§£ç­”æŠ€æœ¯é—®é¢˜ã€‚'",
        height=150
    )
    
    # åº”ç”¨ç³»ç»Ÿæç¤ºè¯
    if st.button("åº”ç”¨æç¤ºè¯"):
        st.session_state["system_prompt"] = system_prompt
        # æ›´æ–°å½“å‰å¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯
        if st.session_state["messages"] and st.session_state["messages"][0]["role"] == "assistant":
            st.session_state["messages"][0]["content"] = system_prompt
        else:
            st.session_state["messages"].insert(0, {"role": "assistant", "content": system_prompt})
        st.rerun()
    
    st.header("å‚æ•°è®¾ç½®")
    
    # æœ€å¤§ç”Ÿæˆé•¿åº¦æ»‘å—
    st.slider(
        "æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max Tokens)",
        min_value=64,
        max_value=512,
        value=st.session_state["max_tokens"],
        step=64,
        help="æ§åˆ¶ç”Ÿæˆå›å¤çš„æœ€å¤§é•¿åº¦ã€‚",
        key="max_tokens_slider",
        on_change=lambda: setattr(st.session_state, "max_tokens", st.session_state.max_tokens_slider)
    )
    
    st.header("èŠå¤©ç®¡ç†")
    
    # æ–°å»ºèŠå¤©æŒ‰é’®
    if st.button("æ–°å»ºèŠå¤©"):
        chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        st.session_state["current_chat"] = chat_id
        st.session_state["messages"] = [
            {"role": "assistant", "content": st.session_state["system_prompt"]}
        ]
        st.rerun()
    
    # æ˜¾ç¤ºèŠå¤©å†å²åˆ—è¡¨
    st.subheader("å†å²èŠå¤©")
    chats = get_chat_list()
    for chat in chats:
        col1, col2 = st.columns([3, 1])
        with col1:
            if st.button(f"ğŸ“ {chat['date']}", key=f"chat_{chat['id']}"):
                st.session_state["current_chat"] = chat["id"]
                loaded_messages = load_chat_history(chat["id"])
                if loaded_messages:
                    st.session_state["messages"] = loaded_messages
                st.rerun()
        with col2:
            if st.button("ğŸ—‘ï¸", key=f"delete_{chat['id']}"):
                file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat['id']}.json")
                if os.path.exists(file_path):
                    os.remove(file_path)
                if st.session_state["current_chat"] == chat["id"]:
                    st.session_state["current_chat"] = None
                    st.session_state["messages"] = [
                        {"role": "assistant", "content": st.session_state["system_prompt"]}
                    ]
                st.rerun()

# ==================== æ¨¡å‹åŠ è½½å’Œæ¨ç† ====================
@st.cache_resource
def load_model(model_path):
    """åŠ è½½æ¨¡å‹å’Œtokenizer
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
    
    Returns:
        tokenizerå’Œmodelçš„å…ƒç»„
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    return tokenizer, model

# åŠ è½½æ¨¡å‹
try:
    model_path = AVAILABLE_MODELS[st.session_state["current_model"]]["path"]
    tokenizer, model = load_model(model_path)
except Exception as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    st.stop()

# ==================== èŠå¤©ç•Œé¢ ====================
# å±•ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    if msg["role"] != "system":
        st.chat_message(msg["role"]).write(msg["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # æ„å»ºå¯¹è¯æ¨¡æ¿
    messages = [{"role": "assistant", "content": st.session_state["system_prompt"]}]
    messages.extend([msg for msg in st.session_state["messages"] if msg["role"] != "assistant"])
    
    prompt_text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)

    # ç”Ÿæˆå›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            outputs = model.generate(
                **inputs,
                max_new_tokens=st.session_state["max_tokens"]
            )
            response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
            st.write(response)
            st.session_state["messages"].append({"role": "assistant", "content": response})
            
            # ä¿å­˜èŠå¤©è®°å½•
            if st.session_state["current_chat"]:
                save_chat_history(st.session_state["current_chat"], st.session_state["messages"])
            else:
                chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.session_state["current_chat"] = chat_id
                save_chat_history(chat_id, st.session_state["messages"]) 