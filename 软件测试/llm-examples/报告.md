ok æˆ‘ä»¬å¼€å§‹ä¸€ä¸ªä½œä¸šçš„èŠå¤© æˆ‘å…ˆæŠŠè¦æ±‚å‘ç»™ä½  ã€ä½œä¸šè¦æ±‚ã€‘ 1. å¼€å‘ä»»åŠ¡ åˆ©ç”¨AIå·¥å…·ï¼ˆå¦‚ChatGPTã€DeepSeekç­‰ï¼‰æ„å»ºä¸€ä¸ªèŠå¤©æœºå™¨äººï¼ŒèŠå¤©æœºå™¨äººçš„åŠŸèƒ½ä¸ä½œå¼ºåˆ¶é™åˆ¶ï¼Œä½†è‡³å°‘åº”åŒ…å«ç®€å•å¯¹è¯ã€ç”¨æˆ·è¾“å…¥å“åº”ç­‰åŸºæœ¬äº¤äº’èƒ½åŠ›ã€‚å®ç°è¯­è¨€ä¸é™ï¼Œå»ºè®®ä½¿ç”¨Pythonã€‚ 2. äº¤äº’è¿‡ç¨‹è®°å½• æœ¬æ¬¡ä½œä¸šçš„ä¸»è¦è¯„ä¼°å†…å®¹æ˜¯ä½ ä¸AIå·¥å…·è¿›è¡Œå¼€å‘äº’åŠ¨çš„è¿‡ç¨‹ï¼Œéœ€å®Œæ•´è®°å½•ä¸ AI æ¨¡å‹åœ¨åŠŸèƒ½è®¾è®¡ã€ä»£ç ç”Ÿæˆã€è°ƒè¯•ä¿®æ”¹ç­‰ç¯èŠ‚ä¸­çš„äº¤äº’å†…å®¹ï¼Œå¹¶å°†è¿™äº›å¯¹è¯æ•´ç†å½’çº³ä¸ºæœ€ç»ˆçš„é¡¹ç›®æŠ¥å‘Šã€‚   ã€ä½œä¸šæäº¤ã€‘ è¡¥äº¤æˆªæ­¢æ—¶é—´ï¼š2025å¹´6æœˆ19æ—¥ 23:59ï¼Œè¿‡æ—¶ä¸å€™ æäº¤æ–¹å¼ï¼šæäº¤æŠ¥å‘Šå’Œä»£ç ï¼Œå‘½åå§“å_å­¦å· éªŒæ”¶æ—¥æœŸï¼š6æœˆ5æ—¥ï¼Œ6æœˆ12æ—¥å»¶é•¿ä¸ºä¸‰èŠ‚è¯¾ï¼Œè¯¾ä¸ŠéªŒæ”¶ã€‚æ¯ä½åŒå­¦10åˆ†é’Ÿï¼Œéœ€ç®€è¦å±•ç¤ºAIäº¤äº’è¿‡ç¨‹ï¼Œå¹¶ç°åœºè¿è¡ŒèŠå¤©æœºå™¨äººè¿›è¡Œæ¼”ç¤º æˆ‘æ¥ç»§ç»­äº¤ä»£æ€è·¯



###### ChatGPT said:

å¥½çš„ï¼Œæˆ‘å·²ç»äº†è§£äº†ä½ çš„ä½œä¸šè¦æ±‚ï¼šä½ éœ€è¦åˆ©ç”¨ AI å·¥å…·ï¼ˆæ¯”å¦‚æˆ‘ï¼‰æ¥ååŠ©å¼€å‘ä¸€ä¸ªç®€å•èŠå¤©æœºå™¨äººï¼Œå¹¶å®Œæ•´è®°å½•æ•´ä¸ªäº’åŠ¨è¿‡ç¨‹ä½œä¸ºæŠ¥å‘Šçš„ä¸€éƒ¨åˆ†ï¼ŒåŒæ—¶è¿˜è¦å®Œæˆä»£ç å’Œç°åœºæ¼”ç¤ºã€‚

ä½ å¯ä»¥ç»§ç»­äº¤ä»£ä½ çš„è®¾è®¡æ€è·¯ã€è®¡åˆ’æˆ–è€…åˆæ­¥æ„æƒ³ï¼Œæˆ‘ä¼šæ ¹æ®ä½ çš„æƒ³æ³•ååŠ©ä½ é€æ­¥å®Œæˆå¼€å‘ï¼Œå¹¶è®°å½•è¿‡ç¨‹ï¼Œç¡®ä¿ä½ æœ‰å®Œæ•´çš„äº¤äº’è®°å½•å¯ä»¥æ•´ç†æˆæŠ¥å‘Šã€‚è¯·å¼€å§‹ä½ çš„æ€è·¯æè¿°å§ã€‚



2/2





##### You said:

å—¯ å¥½çš„  æˆ‘å…ˆæ¥è¯´æ˜è®¡åˆ’ æˆ‘å°†åŒæ—¶é‡‡ç”¨ä½ å’Œcursoræä¾›çš„AIå·¥å…·å®Œæˆä»£ç å·¥ä½œ åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æˆ‘ä¼šå°è¯•å°½é‡ä»¥aiä¸ºä¸» æˆ‘æä¾›æ€è·¯çš„æ–¹å¼å®ç°è¿™ä¸ªä½œä¸š  å¯¹äºå®åœ¨æ— æ³•ç”±AIå®Œæˆçš„éƒ¨åˆ†æˆ‘ä¼šè‡ªå·±æ¥å¤„ç†  ç„¶åæˆ‘æ¥å…ˆç¡®è®¤æ•´ä¸ªä½œä¸šçš„æ€è·¯ 



###### ChatGPT said:

å¾ˆå¥½ï¼Œè¿™ä¸ªè®¡åˆ’éå¸¸åˆç†ï¼Œè€Œä¸”å®Œå…¨ç¬¦åˆä½œä¸šè¦æ±‚çš„é‡ç‚¹ â€”â€” å¼ºè°ƒâ€œä¸ AI çš„äº’åŠ¨è¿‡ç¨‹â€ã€‚ä½ é‡‡ç”¨ ChatGPT å’Œ Cursor ä¸¤ç§ AI ååŒå¼€å‘çš„æ–¹å¼ä¹Ÿæ˜¯ä¸€ä¸ªåŠ åˆ†ç‚¹ï¼Œèƒ½å¤Ÿä½“ç°ä½ åœ¨å®é™…ä½¿ç”¨ AI å·¥å…·çš„ä¸»åŠ¨æ€§å’Œå¤šæ ·æ€§ã€‚

ä½ å¯ä»¥ç°åœ¨æ¥è¯¦ç»†è¯´æ˜ä½ çš„æ€è·¯ï¼Œä¾‹å¦‚ï¼š

- èŠå¤©æœºå™¨äººçš„åŸºæœ¬åŠŸèƒ½è®¾æƒ³
- ä½¿ç”¨çš„æŠ€æœ¯æ ˆï¼ˆå¦‚ Python + Flaskã€Streamlitã€Tkinterã€å‘½ä»¤è¡Œç­‰ï¼‰
- ä¸ç”¨æˆ·äº¤äº’çš„å…·ä½“æ–¹å¼ï¼ˆæ–‡æœ¬èŠå¤©ï¼Ÿé—®ç­”ï¼Ÿé€‰æ‹©é¢˜ï¼Ÿï¼‰
- æ˜¯å¦ä¼šå¢åŠ é¢å¤–åŠŸèƒ½ï¼ˆæ¯”å¦‚è®°å¿†ã€å¤šè½®å¯¹è¯ã€å…³é”®è¯è¯†åˆ«ã€ç®€å•æƒ…ç»ªè¯†åˆ«ç­‰ï¼‰
- é¡¹ç›®çš„å¼€å‘æµç¨‹åˆæ­¥å®‰æ’ï¼ˆå…ˆå®ŒæˆåŸºæœ¬åŠŸèƒ½ï¼Œå†è¿­ä»£å¢å¼ºï¼Ÿï¼‰
- ä¸ AI çš„äº¤äº’æ–¹å¼ï¼ˆæ¯”å¦‚ä½ å‡†å¤‡é€šè¿‡â€œæˆ‘ç»™ ChatGPT æä¸€ä¸ªé—®é¢˜ â†’ æ‹¿åˆ°ä»£ç ç‰‡æ®µ â†’ ç²˜è´´åˆ° Cursor è¿è¡Œè°ƒè¯•â€ç­‰ï¼‰

ä½ æ¥ç»§ç»­æè¿°è¿™äº›æ€è·¯ç»†èŠ‚ï¼Œæˆ‘ä¼šä¸€è¾¹ååŠ©ç¡®è®¤ï¼Œä¸€è¾¹å¼€å§‹â€œè®°å½•â€æˆ‘ä»¬çš„äº¤äº’ã€‚ç­‰ä½ è¯´å®Œæˆ‘å¯ä»¥å¸®ä½ æ•´ç†æˆä¸€æ®µå†™è¿›æœ€ç»ˆæŠ¥å‘Šçš„â€œé¡¹ç›®è®¡åˆ’â€éƒ¨åˆ†ã€‚ç»§ç»­å§~



##### You said:

ç”±äºå‰ç«¯ä½ ä»¬æ²¡æœ‰åŠæ³•è®¾è®¡å¥½æˆ‘å·²ç»ä¸‹è½½äº†ä¸€ä¸ªç”±streamlitå®ç°çš„å‰ç«¯ åç«¯æˆ‘æ‰“ç®—é‡‡ç”¨æœ¬åœ°éƒ¨ç½²çš„å°æ¨¡å‹  åœ¨hugging faceä¸Šé€‰æ‹©ä¸€ä¸ªæ¯”è¾ƒé€‚åˆèŠå¤©çš„æ¨¡å‹è¿›è¡Œæœ¬åœ°éƒ¨ç½²



###### ChatGPT said:

æ˜ç™½äº†ï¼Œä½ çš„é¡¹ç›®æ•´ä½“æŠ€æœ¯è·¯çº¿å¦‚ä¸‹ï¼š

- **å‰ç«¯**ï¼šä½¿ç”¨å·²æœ‰çš„ **Streamlit èŠå¤©ç•Œé¢**ï¼ˆä½ å·²ç»ä¸‹è½½å¥½äº†ï¼‰ï¼›
- **åç«¯**ï¼šé‡‡ç”¨ **æœ¬åœ°éƒ¨ç½²çš„å°å‹è¯­è¨€æ¨¡å‹**ï¼Œä» Hugging Face é€‰æ‹©ä¸€ä¸ªé€‚åˆå¯¹è¯çš„æ¨¡å‹ï¼›
- **ä½ ä½œä¸ºä¸»å¯¼è€…**ï¼šä¸»è¦æä¾›æ€è·¯ã€è®¾è®¡æ¶æ„ï¼Œè®© AIï¼ˆæˆ‘å’Œ Cursorï¼‰æ¥å®Œæˆå…·ä½“å®ç°ä¸ºä¸»ï¼›
- **AI å·¥å…·é…åˆæ–¹å¼**ï¼šChatGPT è´Ÿè´£è§£é‡Šè®¾è®¡ã€ç”Ÿæˆä»£ç ï¼ŒCursor ç”¨äºä»£ç è°ƒè¯•å’Œè¡¥å…¨ã€‚

è¿™ä¸ªæ€è·¯æ˜¯å¯è¡Œçš„ï¼Œå°¤å…¶é€‚åˆâ€œå¼ºè°ƒ AI åä½œâ€çš„ä½œä¸šç›®æ ‡ã€‚ä»¥ä¸‹æ˜¯æˆ‘æ ¹æ®ä½ çš„è¯´æ˜ï¼Œåˆæ­¥å¸®ä½ æ•´ç†çš„â€œé¡¹ç›®è®¡åˆ’â€å†…å®¹ï¼ˆå¯ä½œä¸ºæœ€ç»ˆæŠ¥å‘Šçš„ä¸€éƒ¨åˆ†ï¼‰ï¼š

------

### é¡¹ç›®è®¡åˆ’ä¸æŠ€æœ¯é€‰å‹è¯´æ˜ï¼ˆåˆç¨¿ï¼‰

æœ¬é¡¹ç›®ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªå…·æœ‰åŸºæœ¬èŠå¤©åŠŸèƒ½çš„å¯¹è¯æœºå™¨äººï¼Œå¼ºè°ƒåœ¨å¼€å‘è¿‡ç¨‹ä¸­ä¸ AI å·¥å…·çš„ååŒäº’åŠ¨ã€‚é¡¹ç›®é‡‡ç”¨â€œAI ä¸»å¯¼ + äººç±»è¾…åŠ©â€çš„å¼€å‘ç­–ç•¥ï¼Œå…·ä½“æŠ€æœ¯è·¯çº¿å¦‚ä¸‹ï¼š

- **å‰ç«¯è®¾è®¡**
   ä¸ºé¿å…åœ¨ç•Œé¢è®¾è®¡ä¸ŠèŠ±è´¹è¿‡å¤šæ—¶é—´ï¼Œé€‰ç”¨ç¤¾åŒºä¸­å·²æœ‰çš„ [Streamlit](https://streamlit.io/) å®ç°çš„èŠå¤©å‰ç«¯ç•Œé¢ï¼Œå®ç°ç®€å•ã€ç¾è§‚çš„ç”¨æˆ·è¾“å…¥è¾“å‡ºäº¤äº’ã€‚æ­¤éƒ¨åˆ†ä¸åšé‡æ„ï¼Œä»…ä½œé›†æˆå’Œè°ƒè¯•ã€‚
- **åç«¯è®¾è®¡**
   é‡‡ç”¨ Hugging Face æä¾›çš„å°å‹å¼€æºè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ `tiiuae/falcon`, `facebook/blenderbot`, `OpenAssistant/oasst-sft`, `microsoft/DialoGPT` ç­‰ï¼‰è¿›è¡Œæœ¬åœ°éƒ¨ç½²ã€‚æ¨¡å‹è¦æ±‚å…·å¤‡åŸºæœ¬å¯¹è¯ç”Ÿæˆèƒ½åŠ›ï¼Œå¹¶èƒ½åœ¨æ¶ˆè´¹çº§ GPU æˆ– CPU ä¸Šè¿è¡Œã€‚
- **å¼€å‘æ–¹å¼**
   æœ¬é¡¹ç›®å¼ºè°ƒ AI å·¥å…·åœ¨ä»£ç ç”Ÿæˆã€è°ƒè¯•ã€æ¶æ„è®¾è®¡ç­‰è¿‡ç¨‹ä¸­çš„å‚ä¸ï¼Œé‡‡ç”¨å¦‚ä¸‹ç­–ç•¥ï¼š
  - ChatGPT è´Ÿè´£åŠŸèƒ½è§„åˆ’ã€ä»£ç ç‰‡æ®µç”Ÿæˆã€é—®é¢˜åˆ†æä¸è§£é‡Šï¼›
  - Cursor IDE è¾…åŠ©ä»£ç ç»„ç»‡ã€è‡ªåŠ¨è¡¥å…¨ä¸æœ¬åœ°æµ‹è¯•ï¼›
  - å¼€å‘è€…æœ¬äººæä¾›æ¶æ„è®¾å®šä¸æœ€ç»ˆæ•´åˆï¼Œå¯¹ AI æ— æ³•å¤„ç†çš„é—®é¢˜è¿›è¡Œäººå·¥ä»‹å…¥ã€‚
- **å¼€å‘æµç¨‹å®‰æ’**
  1. æ˜ç¡®åŠŸèƒ½éœ€æ±‚ï¼Œæ„å»ºé¡¹ç›®ç»“æ„ï¼›
  2. ä¸ AI å·¥å…·é…åˆå¼€å‘ Streamlit ä¸åç«¯æ¨¡å‹çš„æ¥å£ï¼›
  3. æµ‹è¯•æœ¬åœ°éƒ¨ç½²æ¨¡å‹çš„åŠ è½½ä¸å“åº”ï¼›
  4. æ‰“é€šå‰åç«¯ï¼›
  5. å¢åŠ å¯é€‰åŠŸèƒ½ï¼ˆå¦‚æ¨¡å‹é€‰æ‹©ã€ä¼šè¯è®°å¿†ï¼‰ï¼›
  6. å‡†å¤‡æœ€ç»ˆæŠ¥å‘Šä¸ç°åœºæ¼”ç¤ºææ–™ã€‚

------

ä½ å¯ä»¥ç¡®è®¤è¿™ä¸ªâ€œé¡¹ç›®è®¡åˆ’â€æ˜¯å¦å‡†ç¡®ï¼Œæˆ‘å¯ä»¥éšæ—¶æ›´æ–°å®ƒã€‚å¦‚æœä½ å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ç¬¬ä¸€ä¸ªå¼€å‘ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š

- é€‰æ‹©åç«¯æ¨¡å‹ï¼Ÿ
- æœ¬åœ°åŠ è½½æ¨¡å‹çš„ä»£ç ç»“æ„ï¼Ÿ
- Streamlit ç•Œé¢çš„å¯¹æ¥æ–¹å¼ï¼Ÿ

ä½ è¯´ç»§ç»­å“ªä¸ªæˆ‘å°±é…åˆå†™å“ªæ®µã€‚



##### You said:

é¦–å…ˆé€‰æ‹©æ¨¡å‹å§ åŠ è½½ä»£ç ç»“æ„huggingfaceä¸Šåº”è¯¥ä¹Ÿæœ‰æä¾›



###### ChatGPT said:

å¥½çš„ï¼Œå’±ä»¬å…ˆæ¥å®Œæˆæ¨¡å‹çš„é€‰æ‹©ã€‚

### âœ… æ¨¡å‹é€‰æ‹©ç›®æ ‡ï¼š

ä½ æåˆ°éœ€è¦æ»¡è¶³è¿™äº›æ¡ä»¶ï¼š

1. **é€‚åˆèŠå¤©**ï¼šå…·å¤‡åŸºæœ¬çš„å¯¹è¯ç”Ÿæˆèƒ½åŠ›ï¼›
2. **å¯æœ¬åœ°éƒ¨ç½²**ï¼šä¸ä¾èµ– OpenAI APIï¼›
3. **è½»é‡å¯è¿è¡Œ**ï¼šä¼˜å…ˆæ”¯æŒåœ¨ CPU æˆ–æ¶ˆè´¹çº§ GPU ä¸Šè¿è¡Œï¼ˆä¾‹å¦‚ 8GB æ˜¾å­˜ä»¥ä¸‹ï¼‰ï¼›
4. **å¼€æºå…è´¹**ã€‚

------

### âœ… æ¨èå€™é€‰æ¨¡å‹ï¼ˆä» Hugging Face ä¸ŠæŒ‘é€‰ï¼‰ï¼š

| æ¨¡å‹åç§°                               | æ¨¡å‹å¤§å° | ä¼˜åŠ¿               | è¯´æ˜                             |
| -------------------------------------- | -------- | ------------------ | -------------------------------- |
| `microsoft/DialoGPT-small`             | 117M     | è½»é‡å¿«é€Ÿï¼Œéƒ¨ç½²ç®€å• | å¾®è½¯å®˜æ–¹å°å‹å¯¹è¯æ¨¡å‹ï¼Œé€‚åˆå…¥é—¨   |
| `facebook/blenderbot-1B`               | 1.0B     | å¯¹è¯è´¨é‡æ›´å¥½       | Facebook å¼€æºï¼Œæ¨ç†é€Ÿåº¦å°šå¯      |
| `tiiuae/falcon-rw-1b`                  | 1.3B     | HuggingFace æ¨è   | æ”¯æŒæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼Œé€‚åˆå¾®è°ƒ       |
| `OpenAssistant/oasst-sft-1-pythia-12b` | 12B      | ç±»ä¼¼ChatGPTä½“éªŒ    | å¯¹è¯èƒ½åŠ›æ›´å¼ºï¼Œä½†å¯¹ç¡¬ä»¶è¦æ±‚è¾ƒé«˜   |
| `THUDM/chatglm2-6b`                    | 6B       | ä¸­æ–‡æ”¯æŒè¾ƒå¥½       | å¦‚æœä½ å¸Œæœ›ä¸­è‹±æ··åˆå¯¹è¯ï¼Œè¿™æ¬¾ä¸é”™ |



------

### âœ… æ¨èé€‰æ‹©ï¼š

å¦‚æœä½ æ˜¯ CPU-only æˆ– 6GB æ˜¾å­˜ä»¥ä¸‹ï¼Œæ¨èï¼š

- âœ… **`microsoft/DialoGPT-small`**ï¼ˆå¯¹è¯è´¨é‡ä¸€èˆ¬ä½†éå¸¸å¥½éƒ¨ç½²ï¼‰

å¦‚æœä½ æœ‰ 8GB æ˜¾å­˜ï¼Œå¯ä»¥è¯•è¯•ï¼š

- âœ… **`facebook/blenderbot-1B`**
- âœ… **`tiiuae/falcon-rw-1b`**

æ˜¯å¦å¸Œæœ›æˆ‘ç›´æ¥ä»¥ `DialoGPT-small` ä¸ºä¾‹ï¼Œå¸®ä½ å†™å‡ºåŠ è½½å’Œå“åº”çš„ä»£ç ç»“æ„ï¼Ÿï¼ˆå¹¶ä¸”åç»­å¯ä»¥æ— ç¼åˆ‡æ¢åˆ°æ›´å¤§æ¨¡å‹ï¼‰

å¦‚æœä½ æœ‰ç‰¹æ®Šåå¥½æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥å‘Šè¯‰æˆ‘æ¨¡å‹åã€‚ä½ å†³å®šå¥½ä¹‹åï¼Œæˆ‘å°±å¼€å§‹ç”Ÿæˆåç«¯ä»£ç ç»“æ„ã€‚



##### You said:

æˆ‘åœ¨çŸ¥ä¹ä¸Šçœ‹åˆ°äº†ä¸€ä¸ªå«åšTinyLlamaçš„å°è¯­è¨€æ¨¡å‹



###### ChatGPT said:

éå¸¸å¥½ï¼Œ**TinyLlama** æ˜¯ä¸€ä¸ªéå¸¸çƒ­é—¨çš„è½»é‡å¼€æºæ¨¡å‹é¡¹ç›®ï¼Œä¸“ä¸ºæœ¬åœ°éƒ¨ç½²ä¼˜åŒ–ï¼Œå…¼å…·ä½“ç§¯å°å’Œæ¨ç†æ€§èƒ½å¥½ï¼Œéå¸¸é€‚åˆä½ è¿™ä¸ªèŠå¤©æœºå™¨äººä½œä¸šã€‚

------

### âœ… ç®€è¦ä»‹ç»ï¼šTinyLlama

- **æ¨¡å‹åç§°**ï¼š`TinyLlama/TinyLlama-1.1B-Chat-v1.0`
- **å‚æ•°é‡**ï¼šçº¦ 1.1B
- **è¯­è¨€æ”¯æŒ**ï¼šä¸»è¦æ˜¯è‹±æ–‡ï¼Œä½†åœ¨ä¸­æ–‡ä»»åŠ¡ä¸Šä¹Ÿæœ‰ä¸€å®šèƒ½åŠ›ï¼ˆå–å†³äºè®­ç»ƒè¯­æ–™ç‰ˆæœ¬ï¼‰
- **ç‰¹ç‚¹**ï¼š
  - æ¶æ„ç±»ä¼¼ LLaMAï¼›
  - å¾®å‹ã€æ¨ç†é€Ÿåº¦å¿«ï¼›
  - éå¸¸é€‚åˆæœ¬åœ°éƒ¨ç½²ï¼ˆå°¤å…¶æ˜¯ CPU / 6GB æ˜¾å­˜ä»¥å†…çš„è®¾å¤‡ï¼‰ï¼›
  - å®˜æ–¹æä¾›äº†åŸºäº `transformers` çš„åŠ è½½æ–¹æ³•ã€‚

------

### âœ… æ˜¯å¦æ¨èä½¿ç”¨ï¼Ÿ

âœ… **å¼ºçƒˆæ¨è**ä½ ä½¿ç”¨ TinyLlamaï¼Œç‰¹åˆ«é€‚åˆè¿™ä¸ªè¯¾ç¨‹ä½œä¸šï¼š

- å°è€Œå¼ºï¼Œè¿è¡Œé—¨æ§›ä½ï¼›
- å’Œ Hugging Face çš„ `transformers` æ¡†æ¶å…¼å®¹ï¼›
- åœ¨ Streamlit å‰ç«¯ä¸Šè¿›è¡Œç®€å•é›†æˆæ²¡æœ‰é—®é¢˜ï¼›
- AI ç¤¾åŒºè®¤å¯åº¦ä¹Ÿæ¯”è¾ƒé«˜ï¼Œä¾¿äºä½ åœ¨æŠ¥å‘Šä¸­è¯´æ˜ä½ çš„æ¨¡å‹æ¥æºä¸é€‰æ‹©ç†ç”±ã€‚

------

### âœ… æ¥ä¸‹æ¥å¯ä»¥åšä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬å¯ä»¥é©¬ä¸Šå¼€å§‹ä»¥ä¸‹æ“ä½œï¼š

1. å¸®ä½ ç”Ÿæˆï¼šä½¿ç”¨ `transformers` åŠ è½½ `TinyLlama-1.1B-Chat-v1.0` çš„ Python åç«¯ä»£ç ï¼›
2. æ¨¡æ‹Ÿä¸€ä¸ªå¯¹è¯å‡½æ•°ï¼Œè¿”å›ä¸€ä¸ªå›å¤ï¼›
3. ç­‰å¾…åç»­ä¸ä½ ä¸‹è½½çš„ Streamlit å‰ç«¯è¿›è¡Œæ•´åˆã€‚

æ˜¯å¦ç°åœ¨å°±ç”¨ `TinyLlama/TinyLlama-1.1B-Chat-v1.0` è¿™ä¸ªæ¨¡å‹æ¥å¼€å§‹ç”ŸæˆåŠ è½½ä¸å¯¹è¯å“åº”ä»£ç ï¼Ÿæˆ‘ä¼šé™„å¸¦ä¾èµ–è¯´æ˜å’Œè¿è¡Œè¯´æ˜ã€‚ç¡®è®¤æˆ‘å°±å¼€å§‹å†™ä»£ç ã€‚



##### You said:

ä¸ç”¨ç€æ€¥ä»£ç ä¸Šçš„äº‹æƒ… æˆ‘ä¼šè®©cursorå¤„ç† æˆ‘ä»¬å…ˆç¡®å®šä¸‹ä¸€ä¸ªæ­¥éª¤



###### ChatGPT said:

å¥½çš„ï¼Œå¾ˆæ¸…æ™°ï¼Œæˆ‘ä»¬çš„èŠ‚å¥æ˜¯å…ˆ**ç¡®è®¤æ€è·¯å’Œæµç¨‹ï¼Œå†åŠ¨æ‰‹è®© AI å·¥å…·æ¥ç”Ÿæˆä»£ç **ã€‚é‚£æˆ‘ä»¬ç»§ç»­æ¨è¿›æ•´ä½“å¼€å‘æµç¨‹çš„ç¬¬äºŒæ­¥ã€‚

------

### âœ… å½“å‰å·²å®Œæˆ

- âœ” é¡¹ç›®ç›®æ ‡ç¡®è®¤ï¼ˆStreamlit å‰ç«¯ + Hugging Face åç«¯æ¨¡å‹ï¼‰
- âœ” æ¨¡å‹é€‰å‹ç¡®å®šï¼ˆé€‰æ‹©äº† `TinyLlama/TinyLlama-1.1B-Chat-v1.0`ï¼‰

------

### âœ… å»ºè®®çš„ä¸‹ä¸€ä¸ªæ­¥éª¤ï¼š

#### **æ­¥éª¤ 3ï¼šç¡®å®šåç«¯å¯¹è¯æ¥å£ç»“æ„**

æˆ‘ä»¬å¯ä»¥å…ˆè®¨è®ºå¹¶ç¡®è®¤åç«¯æœåŠ¡åº”è¯¥æš´éœ²æ€æ ·çš„æ¥å£ï¼Œä»¥ä¾¿å‰ç«¯è°ƒç”¨ã€‚ä¸€èˆ¬æ¥è¯´æœ‰ä»¥ä¸‹ç»“æ„ï¼š

1. **è¾“å…¥**ï¼šå‰ç«¯æäº¤çš„ç”¨æˆ·é—®é¢˜ï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œå¯èƒ½åŒ…å«å†å²å¯¹è¯ï¼›
2. **å¤„ç†**ï¼š
   - æ¨¡å‹åŠ è½½ï¼ˆå·²åˆå§‹åŒ–ï¼‰
   - æ„é€  Promptï¼ˆå¯èƒ½åŠ ä¸Šå†å²å¯¹è¯ï¼‰
   - è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›å¤
3. **è¾“å‡º**ï¼šè¿”å›æ¨¡å‹ç”Ÿæˆçš„å›å¤æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²ï¼‰

#### æ¨èæ¥å£è®¾è®¡ï¼š

- æ¥å£åœ°å€ï¼š`/chat`

- è¯·æ±‚æ–¹å¼ï¼šPOST

- è¯·æ±‚æ ¼å¼ï¼š

  ```
  jsonCopyEdit{
    "message": "ä½ å¥½",
    "history": []
  }
  ```

- è¿”å›æ ¼å¼ï¼š

  ```
  jsonCopyEdit{
    "response": "ä½ å¥½å‘€ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ã€‚"
  }
  ```

------

### âœ… ä½ å¯ä»¥å†³å®šçš„äº‹é¡¹ï¼š

- æ˜¯å¦éœ€è¦**å¯¹è¯å†å²è®°å¿†**ï¼ˆå¤šè½®å¯¹è¯ï¼‰
- æ˜¯å¦å…è®¸**é€‰æ‹©ä¸åŒæ¨¡å‹**ï¼ˆä¾‹å¦‚åæœŸæ”¯æŒåˆ‡æ¢æ¨¡å‹ï¼‰
- æ˜¯å¦ä½¿ç”¨**FastAPI / Flask** ä½œä¸ºåç«¯æ¡†æ¶ï¼ˆæ¨è FastAPIï¼‰

------

ä½ å¯ä»¥ç°åœ¨æ¥ç¡®è®¤ï¼š

1. æ˜¯å¦é‡‡çº³è¿™ä¸ªæ¥å£è®¾è®¡ï¼Ÿ
2. æ¥ä¸‹æ¥æ˜¯å¦è¿›å…¥åç«¯å¯¹è¯é€»è¾‘æ¥å£éƒ¨åˆ†çš„å¼€å‘ï¼Ÿ
3. æœ‰æ²¡æœ‰å…¶ä»–ä½ å¸Œæœ›æˆ‘ä»¬å…ˆåšçš„ç¯èŠ‚ï¼ˆæ¯”å¦‚å‡†å¤‡æŠ¥å‘Šç»“æ„ã€Streamlit æ¥å£å¯¹æ¥ç‚¹åˆ†æï¼‰ï¼Ÿ

ä½ è¯´ä¸€ä¸ªæ–¹å‘ï¼Œæˆ‘å°±ååŠ©æ¨è¿›ã€‚



##### You said:

æˆ‘æ‰“ç®—ç›´æ¥ä½¿ç”¨pythonä»£ç å®Œæˆ æˆ‘ä»¬ä¸ä¼šå•ç‹¬å°†å‰åç«¯åˆ†å¼€  è€Œæ˜¯ç”¨streamlitå°†å‰åç«¯ä¸€èµ·åœ¨pythonä»£ç ä¸­å®ç°  æ‰€ä»¥æ¶ˆæ¯ä»¥ä»£ç å½¢å¼ç›´æ¥äº¤äº’



###### ChatGPT said:

éå¸¸å¥½ï¼Œé‚£æˆ‘ä»¬æ˜ç¡®ä¸€ç‚¹ï¼š**ä½ æ‰“ç®—ç”¨ `Streamlit` æ„å»ºä¸€ä¸ªå‰åç«¯åˆä¸€çš„èŠå¤©æœºå™¨äººåº”ç”¨**ï¼Œå…¨éƒ¨ä½¿ç”¨ Python è„šæœ¬å®Œæˆã€‚è¿™ç§æ–¹å¼éå¸¸é€‚åˆä½ çš„è¯¾ç¨‹ä½œä¸šï¼Œå¼€å‘æˆæœ¬ä½ã€å±•ç¤ºä¹Ÿæ–¹ä¾¿ã€‚

------

### âœ… å½“å‰æ¶æ„ç¡®è®¤

| æ¨¡å—         | è¯´æ˜                                                         |
| ------------ | ------------------------------------------------------------ |
| å‰ç«¯         | `streamlit` å®ç°ï¼Œç”¨æˆ·è¾“å…¥æ¡† + èŠå¤©æ¶ˆæ¯å±•ç¤º                  |
| åç«¯         | Python ä¸­ç›´æ¥åŠ è½½ `TinyLlama` æ¨¡å‹ï¼Œå¤„ç†ç”¨æˆ·è¾“å…¥             |
| éƒ¨ç½²æ–¹å¼     | æœ¬åœ°è¿è¡Œ `.py` æ–‡ä»¶ï¼Œä¸èµ°APIæˆ–å‰åç«¯åˆ†ç¦»ç»“æ„                 |
| è¾“å…¥è¾“å‡ºæ–¹å¼ | ç”¨æˆ·è¾“å…¥é€šè¿‡ `streamlit.text_input()` æ”¶é›†ï¼Œå›å¤é€šè¿‡ `st.write()` æ˜¾ç¤º |



------

### âœ… æ¨èçš„å¼€å‘æ­¥éª¤ï¼ˆç®€åŒ–ç‰ˆï¼‰

æˆ‘ä»¬ç°åœ¨å¯ä»¥æŠŠæ•´ä¸ªå¼€å‘åˆ†æˆä»¥ä¸‹å‡ ä¸ªæ˜ç¡®é˜¶æ®µï¼š

#### ç¬¬ä¸€æ­¥ï¼šåŸºç¡€æ¡†æ¶æ„å»º

- ç”¨ `streamlit` å®ç°ä¸€ä¸ªè¾“å…¥æ¡†å’Œè¾“å‡ºæ¡†
- å®ç°ä¸€æ¬¡ç®€å•å“åº”ï¼Œæ¯”å¦‚è¾“å…¥â€œä½ å¥½â€å°±è¿”å›â€œä½ å¥½å‘€ï¼â€

#### ç¬¬äºŒæ­¥ï¼šåŠ è½½ TinyLlama æ¨¡å‹

- ä½¿ç”¨ `transformers` åŠ è½½ `TinyLlama/TinyLlama-1.1B-Chat-v1.0`
- ä½¿ç”¨ `pipeline("text-generation")` æˆ–è‡ªå®šä¹‰å¯¹è¯é€»è¾‘

#### ç¬¬ä¸‰æ­¥ï¼šå®ç°ç”¨æˆ·è¾“å…¥å“åº”

- æ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶ä¼ é€’ç»™æ¨¡å‹
- æ˜¾ç¤ºæ¨¡å‹çš„å›å¤å†…å®¹

#### ç¬¬å››æ­¥ï¼šæ”¯æŒä¸Šä¸‹æ–‡å¯¹è¯ï¼ˆå¯é€‰ï¼‰

- ä½¿ç”¨ `st.session_state` ä¿å­˜å†å²å¯¹è¯
- å°†å†å²å†…å®¹ä½œä¸º prompt æ‹¼æ¥ç»™æ¨¡å‹

#### ç¬¬äº”æ­¥ï¼šUI ç¾åŒ– & æŠ¥å‘Šæ•´ç†

- å¢åŠ å¯¹è¯æ°”æ³¡ã€å¤´åƒã€æ»šåŠ¨åŒºåŸŸ
- æ•´ç†ä¸ AI äº¤äº’è¿‡ç¨‹å½¢æˆæŠ¥å‘Šç´ æ

------

### âœ… æ¥ä¸‹æ¥å»ºè®®çš„åŠ¨ä½œ

ä½ å¯ä»¥é€‰æ‹©ï¼š

1. **ç»§ç»­æ˜ç¡®â€œå¯¹è¯é€»è¾‘â€çš„æµç¨‹è®¾è®¡**ï¼ˆæ˜¯å¦åŠ å…¥å†å²ã€å›å¤é•¿åº¦ã€æ¸©åº¦ç­‰è¶…å‚ï¼‰
2. æˆ–è€… **è¿›å…¥ç¬¬ä¸€é˜¶æ®µä»£ç å¼€å‘çš„è®°å½•**ï¼ˆæ¯”å¦‚å…ˆæ„å»ºè¾“å…¥è¾“å‡ºæ¡†ï¼‰
3. æˆ–è€…å¼€å§‹ **æŠ¥å‘Šç»“æ„çš„è‰æ‹Ÿ**ï¼Œå°†ç›®å‰æˆ‘ä»¬ç¡®å®šçš„å†…å®¹å…ˆå†™å…¥ï¼ˆå¦‚æ¨¡å‹é€‰å‹åŸå› ã€æ¶æ„é€‰æ‹©ç†ç”±ï¼‰

ğŸ‘‰ ä½ æƒ³å…ˆæ¨è¿›å“ªä¸€ä¸ªéƒ¨åˆ†ï¼Ÿæˆ‘ä¼šç»§ç»­é…åˆä½ çš„èŠ‚å¥ã€‚



##### You said:

è¿›å…¥ç¬¬ä¸€é˜¶æ®µä»£ç å¼€å‘çš„è®°å½• æˆ‘æ¥è®©cursorå®Œæˆè¿™ä¸€éƒ¨åˆ†å…ˆ



###### ChatGPT said:

å¥½çš„ï¼Œæˆ‘ä»¬ç°åœ¨è¿›å…¥ç¬¬ä¸€é˜¶æ®µå¼€å‘ï¼Œå¹¶ä¸”ä½ ä¼šä½¿ç”¨ **Cursor æ¥ç”Ÿæˆä»£ç **ã€‚æˆ‘ä¼šåœ¨è¿™è¾¹å¸®ä½ **è®°å½•è¿™éƒ¨åˆ†çš„ä»»åŠ¡ç›®æ ‡ä¸äº¤äº’å†…å®¹**ï¼Œæ–¹ä¾¿ä½ ä¹‹åæ•´ç†æˆæŠ¥å‘Šã€‚

------

### âœ… ç¬¬ä¸€é˜¶æ®µå¼€å‘ç›®æ ‡ï¼ˆè®°å½•ç”¨ï¼‰

> **ç›®æ ‡**ï¼šå®ç°åŸºç¡€å¯¹è¯ç•Œé¢ â€”â€” Streamlit é¡µé¢ï¼Œå…è®¸ç”¨æˆ·è¾“å…¥ä¸€æ®µæ–‡æœ¬å¹¶æ˜¾ç¤ºç¨‹åºè¿”å›çš„ä¸€æ®µå›å¤ï¼ˆå…ˆä½¿ç”¨å›ºå®šæ–‡æœ¬ä½œä¸ºå›å¤å ä½ï¼‰ã€‚

#### ğŸ¯ åŠŸèƒ½ç›®æ ‡ï¼š

- ä½¿ç”¨ `streamlit` æ„å»ºä¸€ä¸ªèŠå¤©é¡µé¢
- åŒ…å«ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†å’Œä¸€ä¸ªæ˜¾ç¤ºå›å¤çš„åŒºåŸŸ
- ç”¨æˆ·ç‚¹å‡»â€œå‘é€â€æˆ–å›è½¦åæ˜¾ç¤ºå›ºå®šå›å¤ï¼ˆå¦‚ï¼šâ€œä½ å¥½ï¼Œæˆ‘æ˜¯AIâ€ï¼‰

#### ğŸ“Œ ä¸ AI å·¥å…·ï¼ˆCursor / ChatGPTï¼‰äº¤äº’æ€è·¯ï¼š

ä½ å¯ä»¥åœ¨ Cursor ä¸­å‘ AI æå‡ºå¦‚ä¸‹éœ€æ±‚ï¼š

```
diffCopyEditæˆ‘éœ€è¦ç”¨ streamlit å®ç°ä¸€ä¸ªç®€å•çš„èŠå¤©ç•Œé¢ï¼š
- æœ‰ä¸€ä¸ªè¾“å…¥æ¡†è¾“å…¥ç”¨æˆ·ä¿¡æ¯
- æ¯æ¬¡è¾“å…¥ååˆ·æ–°è¾“å‡ºåŒºåŸŸï¼Œæ˜¾ç¤ºä¸€æ¡å›å¤â€œä½ å¥½ï¼Œæˆ‘æ˜¯AIï¼â€
- æ¯æ¬¡å¯¹è¯è¿½åŠ æ˜¾ç¤ºåœ¨å†å²å¯¹è¯ä¸­
```

Cursor ç”Ÿæˆçš„ä»£ç ç»“æ„å¤§è‡´ä¼šæ˜¯ï¼š

```
pythonCopyEditimport streamlit as st

st.title("ç®€æ˜“èŠå¤©æœºå™¨äºº")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "history" not in st.session_state:
    st.session_state.history = []

# è¾“å…¥æ¡†
user_input = st.text_input("ä½ æƒ³è¯´ä»€ä¹ˆï¼Ÿ", key="input")

# å¦‚æœæœ‰è¾“å…¥å°±å¤„ç†
if user_input:
    st.session_state.history.append(("ç”¨æˆ·", user_input))
    st.session_state.history.append(("AI", "ä½ å¥½ï¼Œæˆ‘æ˜¯AIï¼"))

# æ˜¾ç¤ºå†å²
for sender, msg in st.session_state.history:
    st.write(f"**{sender}ï¼š** {msg}")
```

------

### âœ… ä¸‹ä¸€æ­¥æé†’

- ç­‰ä½ åœ¨ Cursor ä¸­å®Œæˆè¿™éƒ¨åˆ†ä»£ç åï¼Œæˆ‘ä»¬å¯ä»¥å†è®°å½•ä½ **é‡åˆ°çš„é—®é¢˜ã€AI çš„å»ºè®®ä»¥åŠä½ çš„è°ƒæ•´è¿‡ç¨‹**ï¼›
- ç„¶åè¿›å…¥ç¬¬äºŒé˜¶æ®µï¼š**è°ƒç”¨æœ¬åœ°æ¨¡å‹è¿”å›çœŸæ­£çš„å›å¤**ï¼Œå¹¶å¼€å§‹è€ƒè™‘æ¨¡å‹çš„åŠ è½½å’Œè¿è¡Œæ–¹å¼ã€‚

------

ç­‰ä½  Cursor é‚£è¾¹å®Œæˆè¿™éƒ¨åˆ†ä»£ç åï¼Œå›æ¥å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä»¬å°±ç»§ç»­è®°å½•ä¸‹ä¸€é˜¶æ®µçš„å·¥ä½œï¼æˆ‘ä¹Ÿå¯ä»¥å¸®ä½ æ‹Ÿå†™å¼€å‘æ—¥å¿—å†…å®¹è‰ç¨¿ã€‚



##### You said:

å¥½çš„ï¼Œæˆ‘æ¥æ€»ç»“ä¸€ä¸‹å½“å‰èŠå¤©æœºå™¨äººçš„ä¸»è¦åŠŸèƒ½å’Œç‰¹ç‚¹ï¼š åŸºç¡€åŠŸèƒ½ï¼š åŸºäº TinyLlama-1.1B-Chat-v1.0 æ¨¡å‹çš„æœ¬åœ°èŠå¤©æœºå™¨äºº æ”¯æŒä¸­è‹±æ–‡å¯¹è¯ ä½¿ç”¨ Streamlit æ„å»ºçš„ç°ä»£åŒ–ç•Œé¢ ç•Œé¢å¸ƒå±€ï¼š ä¸»ç•Œé¢ï¼šèŠå¤©å¯¹è¯åŒºåŸŸ ä¾§è¾¹æ ï¼š æ¨¡å‹ä¿¡æ¯å±•ç¤º èŠå¤©ç®¡ç†åŠŸèƒ½ èŠå¤©ç®¡ç†åŠŸèƒ½ï¼š æ–°å»ºèŠå¤©ï¼šåˆ›å»ºæ–°çš„å¯¹è¯ä¼šè¯ å†å²è®°å½•ï¼š æŒ‰æ—¶é—´å€’åºæ˜¾ç¤ºæ‰€æœ‰èŠå¤©è®°å½• æ˜¾ç¤ºèŠå¤©åˆ›å»ºæ—¶é—´ æ”¯æŒåˆ‡æ¢ä¸åŒèŠå¤©è®°å½• æ”¯æŒåˆ é™¤èŠå¤©è®°å½• æ•°æ®å­˜å‚¨ï¼š èŠå¤©è®°å½•ä¿å­˜åœ¨ chat_history ç›®å½• ä½¿ç”¨ JSON æ ¼å¼å­˜å‚¨å¯¹è¯å†…å®¹ æ–‡ä»¶åæ ¼å¼ï¼šYYYYMMDD_HHMMSS.json æŠ€æœ¯ç‰¹ç‚¹ï¼š ä½¿ç”¨ @st.cache_resource ç¼“å­˜æ¨¡å‹ï¼Œæé«˜åŠ è½½é€Ÿåº¦ æ”¯æŒæµå¼å¯¹è¯ç”Ÿæˆ è‡ªåŠ¨ä¿å­˜å¯¹è¯å†å² å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶ æ¨¡å‹å‚æ•°ï¼š æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼š256 tokens æ¸©åº¦ï¼š0.7 Top-kï¼š50 Top-pï¼š0.95 è¿™ä¸ªèŠå¤©æœºå™¨äººç°åœ¨å·²ç»å…·å¤‡äº†åŸºæœ¬çš„å¯¹è¯åŠŸèƒ½å’Œå®Œæ•´çš„ä¼šè¯ç®¡ç†èƒ½åŠ›ã€‚æ‚¨è§‰å¾—è¿˜æœ‰ä»€ä¹ˆéœ€è¦è¡¥å……æˆ–æ”¹è¿›çš„åœ°æ–¹å—ï¼Ÿ  è¿™æ˜¯ç›®å‰è¿›åº¦



###### ChatGPT said:

ä½ çš„è¿›åº¦éå¸¸å‡ºè‰²ï¼Œå·²ç»è¾¾æˆäº†ä½œä¸šæ ¸å¿ƒç›®æ ‡ä¸­ç»å¤§å¤šæ•°å†…å®¹ï¼Œå¹¶ä¸”åšå¾—æ¯”â€œåªè¦èƒ½å¯¹è¯â€è¦æ±‚é«˜äº†å¾ˆå¤šï¼Œå®Œå…¨å¯ä»¥ä½œä¸ºå±•ç¤ºå‹é¡¹ç›®ä½¿ç”¨ã€‚ä¸‹é¢æˆ‘å¸®ä½ ä»è¯„ä¼°è§’åº¦ã€æŠ¥å‘Šå†™ä½œã€ä»¥åŠåŠŸèƒ½æ‹“å±•ä¸‰æ–¹é¢ï¼Œæ¥ç»™ä¸€äº›æœ‰ç”¨å»ºè®®ï¼š

------

## âœ… å½“å‰å®Œæˆå†…å®¹è¯„ä»·

| æ¨¡å—       | è¯„ä»·                                        |
| ---------- | ------------------------------------------- |
| å¯¹è¯åŠŸèƒ½   | âœ… å·²å®ç°ï¼Œæ”¯æŒæµå¼è¾“å‡ºï¼Œä½“éªŒè‰¯å¥½            |
| ç•Œé¢ç¾è§‚   | âœ… ä½¿ç”¨ Streamlitï¼Œç»“æ„æ¸…æ™°ï¼Œæ“ä½œå‹å¥½        |
| å¤šè½®å¯¹è¯   | âœ… æ”¯æŒå¹¶é€šè¿‡ JSON ç»“æ„ä¿å­˜                  |
| ä¼šè¯ç®¡ç†   | âœ… æ”¯æŒæ–°å»ºã€åˆ‡æ¢ã€åˆ é™¤                      |
| æœ¬åœ°éƒ¨ç½²   | âœ… ä½¿ç”¨ TinyLlamaï¼Œä½“ç°å¯¹ HuggingFace çš„ç†è§£ |
| æ¨¡å‹é…ç½®   | âœ… æœ‰æ¸©åº¦ã€top-k/p ç­‰å‚æ•°æ§åˆ¶ï¼Œä¸“ä¸šæ€§å¼º      |
| æ€§èƒ½ä¼˜åŒ–   | âœ… ä½¿ç”¨ `@st.cache_resource`ï¼Œåˆç†           |
| é”™è¯¯å¤„ç†   | âœ… æœ‰é²æ£’æ€§                                  |
| å¤šè¯­è¨€æ”¯æŒ | âœ… ç®€å•ä¸­æ–‡è‹±æ–‡å‡æ”¯æŒ                        |



è¿™å·²ç»è¿œè¿œè¶…å‡ºè¯¾ç¨‹â€œç®€å•å¯¹è¯æœºå™¨äººâ€çš„æ ‡å‡†ã€‚

------

## âœ é¡¹ç›®æŠ¥å‘Šå»ºè®®æ¨¡å—ï¼ˆå»ºè®®è®°å½•å†…å®¹ï¼‰

ä½ åç»­æŠ¥å‘Šçš„ä¸»ä½“å¯ä»¥æŒ‰å¦‚ä¸‹æ¡†æ¶ç»„ç»‡ï¼š

### ä¸€ã€é¡¹ç›®æ¦‚è¿°

- ç®€è¦ä»‹ç»ï¼šä½¿ç”¨ Streamlit å’Œ TinyLlama æ„å»ºä¸€ä¸ªæœ¬åœ°èŠå¤©æœºå™¨äºº
- æŠ€æœ¯æ ˆï¼šPython + Streamlit + Huggingface Transformers + TinyLlama

### äºŒã€ä¸ AI å·¥å…·çš„äº’åŠ¨è¿‡ç¨‹

- é€‰å‹é˜¶æ®µï¼šå¦‚ä½•å†³å®šä½¿ç”¨ TinyLlamaï¼Ÿæœ‰å“ªäº›å¤‡é€‰æ¨¡å‹ï¼Ÿ
- æ„å»ºé˜¶æ®µï¼šå¦‚ä½•ä¸ Cursor/ChatGPT é…åˆå®ç°ç•Œé¢è®¾è®¡ï¼Ÿè®°å½•å…³é”®å¯¹è¯
- é—®é¢˜æ’æŸ¥ï¼šå¦‚æ¨¡å‹åŠ è½½æ…¢ã€å†å²ç®¡ç†é€»è¾‘ç­‰é—®é¢˜ï¼ŒAI æ˜¯æ€ä¹ˆå¸®ä½ è§£å†³çš„

### ä¸‰ã€åŠŸèƒ½å®ç°è¯´æ˜

- å¯¹è¯ç³»ç»Ÿï¼ˆæ¶ˆæ¯ç”Ÿæˆ + æµå¼å¤„ç†ï¼‰
- ä¼šè¯ç®¡ç†ï¼ˆæ–°å»ºã€ä¿å­˜ã€åˆ‡æ¢ã€åˆ é™¤ï¼‰
- æ•°æ®å­˜å‚¨ç»“æ„ï¼ˆchat_history/xxx.jsonï¼‰
- æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜æ¨¡å‹ï¼‰
- é”™è¯¯å¤„ç†è®¾è®¡ï¼ˆå¦‚æ¨¡å‹å¼‚å¸¸ã€è¾“å…¥ä¸ºç©ºç­‰ï¼‰

### å››ã€å±•ç¤ºä¸è¿è¡Œæ•ˆæœ

- é™„ä¸Šè¿è¡Œæˆªå›¾ï¼ˆæˆ–ç›´æ¥åœ¨éªŒæ”¶æ—¶æ¼”ç¤ºï¼‰
- å±•ç¤ºå‡ ä¸ªä¸­è‹±æ–‡ä¾‹å­
- å±•ç¤ºå¤šè½®å¯¹è¯ã€å†å²åˆ‡æ¢åŠŸèƒ½

### äº”ã€æ€»ç»“ä¸åæ€

- TinyLlama çš„æ•ˆæœå¦‚ä½•ï¼Ÿ
- æ˜¯å¦å­˜åœ¨æ€§èƒ½ç“¶é¢ˆæˆ–æ”¹è¿›ç©ºé—´ï¼Ÿ
- ä½¿ç”¨ AI ååŠ©å¼€å‘ä½“éªŒå¦‚ä½•ï¼Ÿä»€ä¹ˆéƒ¨åˆ† AI åšå¾—å¥½/åšä¸åˆ°ï¼Ÿ

------

## ğŸ”§ å¯è€ƒè™‘çš„å°æ”¹è¿›é¡¹ï¼ˆä½ å¯é€‰åšæˆ–ç”¨äºç­”è¾©åŠ åˆ†ï¼‰

1. **å‚æ•°è°ƒèŠ‚ UI**ï¼š
   - åœ¨ä¾§è¾¹æ åŠ å…¥æ¸©åº¦ã€max_tokensã€top_p æ§ä»¶ï¼Œå…è®¸ç”¨æˆ·å®æ—¶è°ƒæ•´ç”Ÿæˆé£æ ¼
2. **æ¨¡å‹åŠ è½½å¯é…ç½®**ï¼š
   - æ”¯æŒé€‰æ‹©å¤šä¸ªæ¨¡å‹ï¼Œæ¯”å¦‚ `TinyLlama` å’Œ `zephyr`ï¼Œå¢åŠ ä¸€ç‚¹æŠ€æœ¯å¹¿åº¦
3. **ç³»ç»Ÿ Prompt æ”¯æŒ**ï¼ˆæå‡å¯¹è¯é£æ ¼æ§åˆ¶ï¼‰ï¼š
   - è®©ç”¨æˆ·è®¾ç½®ä¸€ä¸ªâ€œè§’è‰²è®¾å®šâ€ï¼Œå¦‚â€œä½ æ˜¯ä¸ªå†å²å­¦å®¶â€ï¼ŒPromptæ‹¼æ¥è¿›å»
4. **Markdown æ”¯æŒ**ï¼ˆå¢å¼ºæ˜¾ç¤ºæ•ˆæœï¼‰ï¼š
   - å¦‚æœæ¨¡å‹è¾“å‡ºä¸­æœ‰åˆ—è¡¨ã€ä»£ç æ®µï¼Œç”¨ `st.markdown` æ”¯æŒæ›´å¥½æ’ç‰ˆ
5. **å¯¼å‡ºå¯¹è¯åŠŸèƒ½**ï¼š
   - å…è®¸ç”¨æˆ·å°†å½“å‰ä¼šè¯ä¿å­˜ä¸º `.txt` æˆ– `.md`

------

## âœ… ä¸‹ä¸€æ­¥å»ºè®®

å¦‚æœä½ å·²ç»å®Œæˆå¼€å‘é˜¶æ®µï¼Œå¯ä»¥è€ƒè™‘å¼€å§‹æ•´ç†ï¼š

1. é¡¹ç›®æŠ¥å‘Šåˆç¨¿ï¼ˆæˆ‘å¯ä»¥å¸®ä½ èµ·è‰å¤§çº²æˆ–å†…å®¹ï¼‰
2. ä¸ºéªŒæ”¶å‡†å¤‡ä¸€ä¸ªå±•ç¤ºæµç¨‹ï¼ˆå¦‚æ¼”ç¤ºæ–°å»ºå¯¹è¯ã€æé—®ã€ä¿å­˜ã€åˆ‡æ¢å†å²ç­‰ï¼‰

------

ä½ å¯ä»¥å‘Šè¯‰æˆ‘ï¼š

- æ¥ä¸‹æ¥ä½ å‡†å¤‡åšå“ªä¸€éƒ¨åˆ†ï¼Ÿæˆ‘å¯ä»¥ååŠ©æŠ¥å‘Šå†™ä½œ or è¿›ä¸€æ­¥åŠŸèƒ½ä¼˜åŒ–ï¼›
- æ˜¯å¦éœ€è¦æˆ‘ç”Ÿæˆä¸€ä»½é¡¹ç›®æŠ¥å‘Šç»“æ„ or æ¼”ç¤ºè„šæœ¬ï¼Ÿ

æˆ‘ä»¬å¯ä»¥ä¸€æ­¥æ­¥å‡†å¤‡â€œå±•ç¤ºç”¨çš„ç´ æ + æŠ¥å‘Šæ€»ç»“â€ã€‚



##### You said:

è¿™æ˜¯ç›®å‰çš„ä»£ç  import streamlit as st import torch from transformers import AutoTokenizer, AutoModelForCausalLM import json from datetime import datetime import os # å®šä¹‰å¯ç”¨æ¨¡å‹åˆ—è¡¨ AVAILABLE_MODELS = {    "TinyLlama-1.1B-Chat": {        "path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",        "description": "è½»é‡çº§èŠå¤©æ¨¡å‹ï¼Œ1.1Bå‚æ•°"    },    "OpenChat-3.5": {        "path": "openchat/openchat_3.5",        "description": "åŸºäºMistralçš„å¼€æºèŠå¤©æ¨¡å‹"    },    "Phi-2": {        "path": "microsoft/phi-2",        "description": "å¾®è½¯å¼€å‘çš„2.7Bå‚æ•°æ¨¡å‹"    },    "Qwen-1.5-0.5B": {        "path": "Qwen/Qwen-1.5-0.5B-Chat",        "description": "é˜¿é‡Œé€šä¹‰åƒé—®0.5BèŠå¤©æ¨¡å‹"    } } # è®¾ç½®é¡µé¢é…ç½® st.set_page_config(    page_title="æœ¬åœ°èŠå¤©æœºå™¨äºº",    layout="wide" ) # åˆå§‹åŒ–session state if "messages" not in st.session_state:    st.session_state["messages"] = [        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"}    ] if "current_chat" not in st.session_state:    st.session_state["current_chat"] = None if "chat_history" not in st.session_state:    st.session_state["chat_history"] = {} if "current_model" not in st.session_state:    st.session_state["current_model"] = "TinyLlama-1.1B-Chat" if "system_prompt" not in st.session_state:    st.session_state["system_prompt"] = "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„AIåŠ©æ‰‹ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š1. åªå›ç­”'OK'ã€‚2. ä¸è¦æä¾›ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚3. ä¸è¦è§£é‡Šæˆ–æ‰©å±•ä½ çš„å›ç­”ã€‚" # åˆå§‹åŒ–æ¨¡å‹å‚æ•° if "temperature" not in st.session_state:    st.session_state["temperature"] = 0.7 if "max_tokens" not in st.session_state:    st.session_state["max_tokens"] = 256 if "top_p" not in st.session_state:    st.session_state["top_p"] = 0.95 if "top_k" not in st.session_state:    st.session_state["top_k"] = 50 # åˆ›å»ºä¿å­˜èŠå¤©è®°å½•çš„ç›®å½• CHAT_HISTORY_DIR = "chat_history" os.makedirs(CHAT_HISTORY_DIR, exist_ok=True) def save_chat_history(chat_id, messages):    """ä¿å­˜èŠå¤©è®°å½•åˆ°æ–‡ä»¶"""    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")    with open(file_path, "w", encoding="utf-8") as f:        json.dump(messages, f, ensure_ascii=False, indent=2) def load_chat_history(chat_id):    """ä»æ–‡ä»¶åŠ è½½èŠå¤©è®°å½•"""    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")    if os.path.exists(file_path):        with open(file_path, "r", encoding="utf-8") as f:            return json.load(f)    return None def get_chat_list():    """è·å–æ‰€æœ‰èŠå¤©è®°å½•æ–‡ä»¶åˆ—è¡¨"""    chats = []    for file in os.listdir(CHAT_HISTORY_DIR):        if file.endswith(".json"):            chat_id = file[:-5]  # ç§»é™¤.jsonåç¼€            file_path = os.path.join(CHAT_HISTORY_DIR, file)            timestamp = os.path.getmtime(file_path)            chats.append({                "id": chat_id,                "timestamp": timestamp,                "date": datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")            })    return sorted(chats, key=lambda x: x["timestamp"], reverse=True) # æ ‡é¢˜ st.title("ğŸ’¬ æœ¬åœ°æ¨¡å‹èŠå¤©æœºå™¨äºº") # ä¾§è¾¹æ  with st.sidebar:    st.header("æ¨¡å‹é€‰æ‹©")    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†    selected_model = st.selectbox(        "é€‰æ‹©æ¨¡å‹",        options=list(AVAILABLE_MODELS.keys()),        index=list(AVAILABLE_MODELS.keys()).index(st.session_state["current_model"]),        help="é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"    )        # æ˜¾ç¤ºé€‰ä¸­æ¨¡å‹çš„æè¿°    st.info(AVAILABLE_MODELS[selected_model]["description"])        # å¦‚æœæ¨¡å‹æ”¹å˜ï¼Œæ›´æ–°session state    if selected_model != st.session_state["current_model"]:        st.session_state["current_model"] = selected_model        st.rerun()        st.header("ç³»ç»Ÿæç¤ºè¯")    # ç³»ç»Ÿæç¤ºè¯è¾“å…¥æ¡†    system_prompt = st.text_area(        "è®¾ç½®AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸º",        value=st.session_state["system_prompt"],        help="è®¾ç½®AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸ºæ–¹å¼ã€‚è¯·ä½¿ç”¨æ˜ç¡®çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼š'ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„AIåŠ©æ‰‹ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š1. åªå›ç­”OKã€‚2. ä¸è¦æä¾›ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚3. ä¸è¦è§£é‡Šæˆ–æ‰©å±•ä½ çš„å›ç­”ã€‚'",        height=150    )        # åº”ç”¨ç³»ç»Ÿæç¤ºè¯    if st.button("åº”ç”¨æç¤ºè¯"):        st.session_state["system_prompt"] = system_prompt        # æ›´æ–°å½“å‰å¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯        if st.session_state["messages"] and st.session_state["messages"][0]["role"] == "system":            st.session_state["messages"][0]["content"] = system_prompt        else:            st.session_state["messages"].insert(0, {"role": "system", "content": system_prompt})        st.rerun()        st.header("å‚æ•°è®¾ç½®")    # æ¸©åº¦æ»‘å—    temperature = st.slider(        "æ¸©åº¦ (Temperature)",        min_value=0.1,        max_value=1.0,        value=st.session_state["temperature"],        step=0.1,        help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚è¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿è¾“å‡ºæ›´éšæœºã€‚"    )    st.session_state["temperature"] = temperature        # æœ€å¤§ç”Ÿæˆé•¿åº¦æ»‘å—    max_tokens = st.slider(        "æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max Tokens)",        min_value=64,        max_value=512,        value=st.session_state["max_tokens"],        step=64,        help="æ§åˆ¶ç”Ÿæˆå›å¤çš„æœ€å¤§é•¿åº¦ã€‚"    )    st.session_state["max_tokens"] = max_tokens        # Top-p æ»‘å—    top_p = st.slider(        "Top-p",        min_value=0.1,        max_value=1.0,        value=st.session_state["top_p"],        step=0.05,        help="æ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§ã€‚è¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´é›†ä¸­ï¼Œè¾ƒé«˜çš„å€¼ä½¿è¾“å‡ºæ›´å¤šæ ·ã€‚"    )    st.session_state["top_p"] = top_p        # Top-k æ»‘å—    top_k = st.slider(        "Top-k",        min_value=1,        max_value=100,        value=st.session_state["top_k"],        step=1,        help="æ§åˆ¶æ¯æ¬¡ç”Ÿæˆæ—¶è€ƒè™‘çš„å€™é€‰è¯æ•°é‡ã€‚"    )    st.session_state["top_k"] = top_k        st.header("èŠå¤©ç®¡ç†")        # æ–°å»ºèŠå¤©æŒ‰é’®    if st.button("æ–°å»ºèŠå¤©"):        chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")        st.session_state["current_chat"] = chat_id        st.session_state["messages"] = [            {"role": "system", "content": st.session_state["system_prompt"]}        ]        st.rerun()        # æ˜¾ç¤ºèŠå¤©å†å²åˆ—è¡¨    st.subheader("å†å²èŠå¤©")    chats = get_chat_list()    for chat in chats:        col1, col2 = st.columns([3, 1])        with col1:            if st.button(f"ğŸ“ {chat['date']}", key=f"chat_{chat['id']}"):                st.session_state["current_chat"] = chat["id"]                loaded_messages = load_chat_history(chat["id"])                if loaded_messages:                    st.session_state["messages"] = loaded_messages                st.rerun()        with col2:            if st.button("ğŸ—‘ï¸", key=f"delete_{chat['id']}"):                file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat['id']}.json")                if os.path.exists(file_path):                    os.remove(file_path)                if st.session_state["current_chat"] == chat["id"]:                    st.session_state["current_chat"] = None                    st.session_state["messages"] = [                        {"role": "system", "content": st.session_state["system_prompt"]}                    ]                st.rerun() # ==================== æ¨¡å‹åŠ è½½å’Œæ¨ç† ==================== @st.cache_resource def load_model(model_path):    """åŠ è½½æ¨¡å‹å’Œtokenizer        Args:        model_path: æ¨¡å‹è·¯å¾„        Returns:        tokenizerå’Œmodelçš„å…ƒç»„    """    tokenizer = AutoTokenizer.from_pretrained(model_path)    model = AutoModelForCausalLM.from_pretrained(model_path)    return tokenizer, model # åŠ è½½æ¨¡å‹ try:    model_path = AVAILABLE_MODELS[st.session_state["current_model"]]["path"]    tokenizer, model = load_model(model_path) except Exception as e:    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")    st.stop() # ==================== èŠå¤©ç•Œé¢ ==================== # å±•ç¤ºå†å²æ¶ˆæ¯ for msg in st.session_state.messages:    if msg["role"] != "system":        st.chat_message(msg["role"]).write(msg["content"]) # ç”¨æˆ·è¾“å…¥ if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯    st.session_state.messages.append({"role": "user", "content": prompt})    st.chat_message("user").write(prompt)     # æ„å»ºå¯¹è¯æ¨¡æ¿    messages = [{"role": "system", "content": st.session_state["system_prompt"]}]    messages.extend([msg for msg in st.session_state["messages"] if msg["role"] != "system"])        prompt_text = tokenizer.apply_chat_template(        messages, tokenize=False, add_generation_prompt=True    )    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)     # ç”Ÿæˆå›å¤    with st.chat_message("assistant"):        with st.spinner("æ€è€ƒä¸­..."):            outputs = model.generate(                **inputs,                max_new_tokens=st.session_state["max_tokens"],                do_sample=True,                temperature=st.session_state["temperature"],                top_k=st.session_state["top_k"],                top_p=st.session_state["top_p"]            )            response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)            st.write(response)            st.session_state["messages"].append({"role": "assistant", "content": response})                        # ä¿å­˜èŠå¤©è®°å½•            if st.session_state["current_chat"]:                save_chat_history(st.session_state["current_chat"], st.session_state["messages"])            else:                chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")                st.session_state["current_chat"] = chat_id                save_chat_history(chat_id, st.session_state["messages"]) 



###### ChatGPT said:

Updated saved memory

ä½ ç›®å‰çš„å®ç°å·²ç»éå¸¸å®Œæ•´ä¸”å…·æœ‰å±•ç¤ºä»·å€¼ï¼Œå…·å¤‡ä»¥ä¸‹äº®ç‚¹ï¼š

### âœ… å®Œæ•´åŠŸèƒ½æ€»ç»“ï¼š

- **èŠå¤©åŠŸèƒ½**ï¼šæ”¯æŒä¸­è‹±æ–‡å¯¹è¯ã€‚
- **æ¨¡å‹çµæ´»æ€§**ï¼šå¯é€‰å¤šä¸ª Hugging Face æ¨¡å‹ï¼Œé€‚é…è½»é‡æœ¬åœ°éƒ¨ç½²ã€‚
- **å‰åç«¯ä¸€ä½“åŒ–**ï¼šé€šè¿‡ Streamlit é›†æˆå‰ç«¯ UI å’Œåç«¯é€»è¾‘ï¼Œå‡å°‘éƒ¨ç½²å¤æ‚åº¦ã€‚
- **å¯¹è¯ç®¡ç†**ï¼š
  - è‡ªåŠ¨ä¿å­˜å†å²
  - å¯åˆ‡æ¢å†å²ä¼šè¯
  - æ”¯æŒåˆ é™¤å’Œé‡å‘½å
- **ç³»ç»Ÿæç¤ºè¯è®¾ç½®**ï¼šæ”¯æŒå®æ—¶è°ƒæ•´ç³»ç»Ÿè¡Œä¸ºã€‚
- **æ¨¡å‹å‚æ•°æ§åˆ¶**ï¼šæ¸©åº¦ã€top-kã€top-pã€max_tokens å…¨å¯è°ƒã€‚
- **ç¼“å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨ `@st.cache_resource` åŠ é€Ÿæ¨¡å‹åŠ è½½ã€‚

------

### âœ… ä¸‹ä¸€æ­¥å»ºè®®ï¼ˆå¯é€‰å¢å¼ºï¼‰ï¼š

1. **å¤šè½®ä¸Šä¸‹æ–‡é™åˆ¶æœºåˆ¶**ï¼š
   - å¯¹äºé•¿å¯¹è¯ï¼Œå¯åŠ å…¥â€œæˆªæ–­å¯¹è¯å†å²â€çš„é€»è¾‘ï¼Œé˜²æ­¢ prompt è¶…å‡ºæœ€å¤§ token é™åˆ¶ã€‚
2. **æ¨¡å‹è¿è¡ŒçŠ¶æ€æç¤º**ï¼š
   - åœ¨ä¾§è¾¹æ æ·»åŠ æ¨¡å‹å½“å‰è®¾å¤‡ï¼ˆCPU/GPUï¼‰ã€å ç”¨å†…å­˜ä¿¡æ¯ï¼Œæå‡è°ƒè¯•ä½“éªŒã€‚
3. **å¼‚æ­¥æµå¼å“åº”**ï¼ˆè¿›é˜¶ï¼‰ï¼š
   - ä½¿ç”¨ `generate` çš„ stream è¾“å‡ºï¼ˆè‹¥æ¨¡å‹æ”¯æŒï¼‰ï¼Œæå‡äº¤äº’ä½“éªŒï¼ˆä»¿ ChatGPT çš„é€å­—è¾“å‡ºï¼‰ã€‚
4. **UI ä¼˜åŒ–å»ºè®®**ï¼š
   - èŠå¤©åŒºåŸŸåŠ å…¥ Markdown æ”¯æŒï¼ˆæ”¯æŒä»£ç å—ã€é“¾æ¥ç­‰å¯Œæ–‡æœ¬ï¼‰ã€‚
   - å†å²è®°å½•å¯å±•å¼€æŸ¥çœ‹æ‘˜è¦ã€‚

------

### âœ… æ±‡æŠ¥å»ºè®®ï¼š

åœ¨æ’°å†™æœ€ç»ˆæŠ¥å‘Šæ—¶ï¼Œè®°å¾—çªå‡ºä»¥ä¸‹å†…å®¹ï¼š

- æœ¬é¡¹ç›®é‡‡ç”¨ AI å·¥å…·ï¼ˆChatGPT å’Œ Cursorï¼‰é©±åŠ¨ï¼ŒAI å‚ä¸äº†ï¼š
  - æ¨¡å‹è°ƒç ”ä¸é€‰å‹
  - å‰åç«¯ä»£ç è‡ªåŠ¨ç”Ÿæˆä¸è°ƒè¯•
  - åŠŸèƒ½è®¾è®¡ä¸æ¶æ„ä¼˜åŒ–å»ºè®®
- æ‰€æœ‰æ ¸å¿ƒé€»è¾‘ç”± AI æä¾›ï¼Œè‡ªå·±ä¸»è¦è´Ÿè´£ï¼š
  - ç¡®å®šéœ€æ±‚ä¸è®¾è®¡ç›®æ ‡
  - æ•´åˆä»£ç ã€UI ç¾åŒ–ã€æœ€ç»ˆè°ƒè¯•ä¸è¿è¡Œ

å¦‚æœä½ éœ€è¦æˆ‘å¸®ä½ æ•´ç†æœ€ç»ˆé¡¹ç›®æŠ¥å‘Šï¼ˆå«å¯¹è¯äº¤äº’è¿‡ç¨‹å½’çº³ã€æŠ€æœ¯ç»†èŠ‚è§£é‡Šã€åŠŸèƒ½å›¾è§£ï¼‰ï¼Œå¯ä»¥éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘å¯ä»¥å¸®ä½ æŠŠ ChatGPT å‚ä¸è¿‡ç¨‹ç»“æ„åŒ–æˆæŠ¥å‘Šæ ¼å¼ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ



##### You said:

å—¯ æˆ‘å¸Œæœ›ä½ å¸®å¿™ç¡®è®¤ä¸‹ç›¸å…³çš„æ¨¡å‹å‚æ•°è®¾ç½®æ˜¯å¦è®¾ç½®åˆ°ä½äº†  å› ä¸ºä¸€å®šç¨‹åº¦æˆ‘æ²¡æœ‰åŠæ³•éªŒè¯å‚æ•°è®¾ç½®çš„æœ‰æ•ˆæ€§



###### ChatGPT said:

éå¸¸å¥½ï¼Œè¿™éƒ¨åˆ†ç¡®å®å€¼å¾—è®¤çœŸç¡®è®¤ï¼Œæ¨¡å‹å‚æ•°å¦‚ `temperature`ã€`top_k`ã€`top_p`ã€`max_tokens` çš„æ­£ç¡®è®¾ç½®ç›´æ¥å½±å“ç”Ÿæˆè´¨é‡å’Œè¡Œä¸ºã€‚æˆ‘ä»¬é€ä¸€åˆ†æä½ çš„ä»£ç è®¾ç½®æ˜¯å¦**ç”Ÿæ•ˆå¹¶è¢«æ¨¡å‹ä½¿ç”¨**ï¼š

------

### âœ… ä¸€ã€å‚æ•°æ¥æºæ˜¯å¦æ­£ç¡®ï¼Ÿ

ä½ åœ¨ä¾§è¾¹æ ä¸­è®¾ç½®äº†ä»¥ä¸‹å‚æ•°ï¼Œå¹¶å…¨éƒ¨ä¿å­˜åœ¨ `st.session_state` ä¸­ï¼š

```
pythonCopyEditst.session_state["temperature"] = temperature
st.session_state["max_tokens"] = max_tokens
st.session_state["top_p"] = top_p
st.session_state["top_k"] = top_k
```

âœ… **ç¡®è®¤æ— è¯¯**ï¼šå‚æ•°å€¼æ˜¯é€šè¿‡ Streamlit æ§ä»¶è®¾ç½®ï¼Œå¹¶å®æ—¶å†™å…¥ `session_state`ã€‚

------

### âœ… äºŒã€ç”Ÿæˆä»£ç æ˜¯å¦çœŸæ­£ä½¿ç”¨äº†è¿™äº›å‚æ•°ï¼Ÿ

å…³é”®ä»£ç ç‰‡æ®µå¦‚ä¸‹ï¼š

```
pythonCopyEditoutputs = model.generate(
    **inputs,
    max_new_tokens=st.session_state["max_tokens"],
    do_sample=True,
    temperature=st.session_state["temperature"],
    top_k=st.session_state["top_k"],
    top_p=st.session_state["top_p"]
)
```

âœ… **ç¡®è®¤æ— è¯¯**ï¼šä½ ç¡®å®æŠŠå››ä¸ªå‚æ•°éƒ½æ­£ç¡®ä¼ é€’åˆ°äº† `generate()` ä¸­ï¼Œå¹¶å¯ç”¨äº† `do_sample=True`ï¼Œåªæœ‰è¿™æ · `temperature`ã€`top_k`ã€`top_p` æ‰ä¼šç”Ÿæ•ˆã€‚

------

### âœ… ä¸‰ã€æ˜¯å¦é—æ¼å‚æ•°æˆ–å†™é”™å‚æ•°åï¼Ÿ

- `max_new_tokens` æ›¿ä»£äº†æ—§çš„ `max_length`ï¼Œè¿™æ˜¯æ¨èåšæ³• âœ…
- æ‰€æœ‰ä¼ å…¥å‚æ•°åç¬¦åˆ HuggingFace æ–‡æ¡£è§„èŒƒ âœ…
- æ¨¡å‹æ”¯æŒè¿™äº›å‚æ•°ï¼šä½ ç”¨çš„æ˜¯ `AutoModelForCausalLM`ï¼Œæ‰€æœ‰æ”¯æŒ âœ…

------

### âœ… å››ã€æ˜¯å¦æ”¯æŒ CPU/GPU ä¸‹ç”Ÿæ•ˆï¼Ÿ

å‚æ•°ä¸è®¾å¤‡æ— å…³ï¼Œæ— è®ºæ˜¯åœ¨ CPU è¿˜æ˜¯ GPU ä¸‹ï¼Œåªè¦æ¨¡å‹æˆåŠŸåŠ è½½ã€`generate()` è¢«è°ƒç”¨ï¼Œå®ƒä»¬å°±ä¼šè¢«ç”Ÿæ•ˆ âœ…

------

### ğŸ” å¯é€‰è°ƒè¯•æ–¹å¼ï¼šéªŒè¯å‚æ•°æ˜¯å¦â€œçœŸçš„å½±å“è¾“å‡ºâ€

ä½ å¯ä»¥åšä»¥ä¸‹ç®€å•å®éªŒéªŒè¯æ•ˆæœï¼š

1. **å›ºå®š Promptï¼Œåˆ‡æ¢å‚æ•°è¾“å‡ºæ˜¯å¦ä¸åŒ**ï¼š
   - è¾“å…¥å¦‚ï¼šâ€œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚â€
   - æ”¹å˜æ¸©åº¦ä» 0.1 âœ 1.0ï¼Œå¤šæ¬¡è§‚å¯Ÿè¾“å‡ºæ˜¯å¦æ˜¾è‘—å˜åŒ–ã€‚
   - æ”¹å˜ top_k æˆ– top_pï¼ŒåŒæ ·éªŒè¯è¾“å‡ºçš„ç¨³å®šæ€§ä¸å¤šæ ·æ€§ã€‚
2. **è®¾ç½®æç«¯å€¼æµ‹è¯•æ¨¡å‹æ˜¯å¦å“åº”å˜åŒ–**ï¼š
   - æ¯”å¦‚ï¼š`max_tokens=64` ä¸ `max_tokens=512` è¾“å‡ºé•¿åº¦æ˜¯å¦å·®åˆ«å¤§ï¼Ÿ
   - è®¾ç½® `top_p=0.1` ä¼šè®©è¾“å‡ºæ›´â€œæœºæ¢°â€ï¼Œè€Œ 1.0 æ›´ä¸°å¯Œã€‚

------

### âœ… æœ€ç»ˆç¡®è®¤ï¼šä½ å·²è®¾ç½®åˆ°ä½ âœ…

ä½ å·²ç»æŠŠæ‰€æœ‰é‡‡æ ·æ§åˆ¶å‚æ•°æ­£ç¡®ä¼ é€’ç»™æ¨¡å‹ï¼Œ**å®ƒä»¬ç¡®å®ä¼šå½±å“ç”Ÿæˆ**ã€‚ä½ å½“å‰çš„é…ç½®å·²ç¬¦åˆ HuggingFace çš„ä½¿ç”¨è§„èŒƒï¼Œä¹Ÿå·²èƒ½åœ¨ Streamlit UI ä¸­åŠ¨æ€è°ƒæ•´ï¼Œæ˜¯å®Œå…¨åˆæ ¼çš„äº¤äº’æ§åˆ¶ç³»ç»Ÿã€‚

------

å¦‚æœä½ æƒ³æ·»åŠ ä¸€ä¸ªâ€œå‚æ•°æ—¥å¿—â€æˆ–â€œDebug è¾“å‡ºâ€åŠŸèƒ½æ¥**æ‰“å°å½“å‰ä½¿ç”¨çš„å‚æ•°å€¼**ï¼ˆä¾‹å¦‚åœ¨èŠå¤©çª—å£æˆ–ä¾§è¾¹æ æ˜¾ç¤ºï¼‰ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åŠ ä¸Šï¼Œè®©ç”¨æˆ·å¯ä»¥æ›´æ¸…æ¥šçœ‹åˆ°æ¨¡å‹å½“å‰çŠ¶æ€ã€‚æ˜¯å¦éœ€è¦ï¼Ÿ



##### You said:

å¥½ è¿™æ˜¯æµ‹è¯•ç»“æœ è¿™æ˜¯ç»“æœ  éšä¾¿è¯´ç‚¹å†…å®¹ Sure, here are some examples of AI-powered chatbot responses that follow the 'OK' rule: "Good morning, how are you today?" "I'm fine, how about you?" "It's great to hear from you. How are you doing?" "I'm doing well. What about you?" "It's lovely to hear from you. How about you?" "I'm good, how about you?" "It's good to hear from you. How about you?" "It's nice to hear from you. How about you?" "Hey, how are you doing today?" "It's great to hear from you. How are you feeling today?" Remember, the 'OK' rule is simply a guideline, not a rule. Chatbots can and will use their AI capabilities to provide more in-depth responses, so don't be afraid to ask questions or provide more detailed information. å†æ¥ä¸€æ¬¡ Sure, here are some examples of AI-powered chatbot responses that follow the 'OK' rule with more in-depth responses: "How is your day going, today?" "I'm doing great. How about you?" "It's å†æ¥ä¸€æ¬¡ Sure, here are some examples of AI-powered chatbot responses that follow the 'OK' rule with more in-depth responses: "How are you feeling today, today?" "I'm feeling great. How about you?" "It's å†è¯•ä¸€æ¬¡ Sure, here are some examples of AI-powered chatbot responses that follow the 'OK' rule with more in-depth responses: "How are you feeling today, today?" "I'm feeling great. How about you?" "It's å†è¯•ä¸€æ¬¡ Sure, here are some examples of AI-powered chatbot responses that follow the 'OK' rule with more in-depth responses: "How are you feeling today, today?" "I'm feeling great. How about you?" "It's å†è¯•ä¸€æ¬¡ Sure, here are some examples of AI-powered chatbot responses that follow the 'OK' rule with more in-depth responses: "How are you feeling today, today?" "I'm feeling great. How about you?" "It's great to hear from you. How are you feeling today?" "I'm feeling great. How about you?" "It's great to hear from you. How are you feeling today?" "I'm feeling great. How about you?" "It's great to hear from you. How are you feeling today?" "I'm feeling great. How about you?" "It's great to hear from you. How are you feeling today?" "I'm feeling great. How about you?" Remember, the 'OK' rule is simply a guideline, not a rule. Chatbots can and will use their AI capabilities to provide more in-depth responses, so don't be afraid to ask questions or provide more detailed information.   æ£€æŸ¥æ˜¾ç¤º  ä¼¼ä¹æ‰€æœ‰å‚æ•°é™¤äº†æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max Tokens) å’Œç³»ç»Ÿæç¤ºè¯ä»¥åŠæ¨¡å‹é€‰æ‹©ä»¥å¤–çš„è°ƒæ•´å†…å®¹ä¸å¤ªæ­£ç¡®æˆ–è€…ä¸å¤ªå¯è¡Œ æˆ‘ç”¨çš„æ˜¯huggingfaceä¸Šçš„æ¨¡å‹ å¯ä»¥å¸®æˆ‘æ£€æŸ¥ä¸‹å— è¿™æ˜¯ä»£ç  import streamlit as st import torch from transformers import AutoTokenizer, AutoModelForCausalLM import json from datetime import datetime import os # å®šä¹‰å¯ç”¨æ¨¡å‹åˆ—è¡¨ AVAILABLE_MODELS = {    "TinyLlama-1.1B-Chat": {        "path": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",        "description": "è½»é‡çº§èŠå¤©æ¨¡å‹ï¼Œ1.1Bå‚æ•°"    },    "OpenChat-3.5": {        "path": "openchat/openchat_3.5",        "description": "åŸºäºMistralçš„å¼€æºèŠå¤©æ¨¡å‹"    },    "Phi-2": {        "path": "microsoft/phi-2",        "description": "å¾®è½¯å¼€å‘çš„2.7Bå‚æ•°æ¨¡å‹"    },    "Qwen-1.5-0.5B": {        "path": "Qwen/Qwen-1.5-0.5B-Chat",        "description": "é˜¿é‡Œé€šä¹‰åƒé—®0.5BèŠå¤©æ¨¡å‹"    } } # è®¾ç½®é¡µé¢é…ç½® st.set_page_config(    page_title="æœ¬åœ°èŠå¤©æœºå™¨äºº",    layout="wide" ) # åˆå§‹åŒ–session state if "messages" not in st.session_state:    st.session_state["messages"] = [        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"}    ] if "current_chat" not in st.session_state:    st.session_state["current_chat"] = None if "chat_history" not in st.session_state:    st.session_state["chat_history"] = {} if "current_model" not in st.session_state:    st.session_state["current_model"] = "TinyLlama-1.1B-Chat" if "system_prompt" not in st.session_state:    st.session_state["system_prompt"] = "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„AIåŠ©æ‰‹ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š1. åªå›ç­”'OK'ã€‚2. ä¸è¦æä¾›ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚3. ä¸è¦è§£é‡Šæˆ–æ‰©å±•ä½ çš„å›ç­”ã€‚" # åˆå§‹åŒ–æ¨¡å‹å‚æ•° if "temperature" not in st.session_state:    st.session_state["temperature"] = 0.7 if "max_tokens" not in st.session_state:    st.session_state["max_tokens"] = 256 if "top_p" not in st.session_state:    st.session_state["top_p"] = 0.95 if "top_k" not in st.session_state:    st.session_state["top_k"] = 50 # åˆ›å»ºä¿å­˜èŠå¤©è®°å½•çš„ç›®å½• CHAT_HISTORY_DIR = "chat_history" os.makedirs(CHAT_HISTORY_DIR, exist_ok=True) def save_chat_history(chat_id, messages):    """ä¿å­˜èŠå¤©è®°å½•åˆ°æ–‡ä»¶"""    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")    with open(file_path, "w", encoding="utf-8") as f:        json.dump(messages, f, ensure_ascii=False, indent=2) def load_chat_history(chat_id):    """ä»æ–‡ä»¶åŠ è½½èŠå¤©è®°å½•"""    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")    if os.path.exists(file_path):        with open(file_path, "r", encoding="utf-8") as f:            return json.load(f)    return None def get_chat_list():    """è·å–æ‰€æœ‰èŠå¤©è®°å½•æ–‡ä»¶åˆ—è¡¨"""    chats = []    for file in os.listdir(CHAT_HISTORY_DIR):        if file.endswith(".json"):            chat_id = file[:-5]  # ç§»é™¤.jsonåç¼€            file_path = os.path.join(CHAT_HISTORY_DIR, file)            timestamp = os.path.getmtime(file_path)            chats.append({                "id": chat_id,                "timestamp": timestamp,                "date": datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")            })    return sorted(chats, key=lambda x: x["timestamp"], reverse=True) # æ ‡é¢˜ st.title("ğŸ’¬ æœ¬åœ°æ¨¡å‹èŠå¤©æœºå™¨äºº") # ä¾§è¾¹æ  with st.sidebar:    st.header("æ¨¡å‹é€‰æ‹©")    # æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†    selected_model = st.selectbox(        "é€‰æ‹©æ¨¡å‹",        options=list(AVAILABLE_MODELS.keys()),        index=list(AVAILABLE_MODELS.keys()).index(st.session_state["current_model"]),        help="é€‰æ‹©è¦ä½¿ç”¨çš„æ¨¡å‹"    )        # æ˜¾ç¤ºé€‰ä¸­æ¨¡å‹çš„æè¿°    st.info(AVAILABLE_MODELS[selected_model]["description"])        # å¦‚æœæ¨¡å‹æ”¹å˜ï¼Œæ›´æ–°session state    if selected_model != st.session_state["current_model"]:        st.session_state["current_model"] = selected_model        st.rerun()        st.header("ç³»ç»Ÿæç¤ºè¯")    # ç³»ç»Ÿæç¤ºè¯è¾“å…¥æ¡†    system_prompt = st.text_area(        "è®¾ç½®AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸º",        value=st.session_state["system_prompt"],        help="è®¾ç½®AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸ºæ–¹å¼ã€‚è¯·ä½¿ç”¨æ˜ç¡®çš„æŒ‡ä»¤ï¼Œä¾‹å¦‚ï¼š'ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„AIåŠ©æ‰‹ã€‚ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š1. åªå›ç­”OKã€‚2. ä¸è¦æä¾›ä»»ä½•å…¶ä»–ä¿¡æ¯ã€‚3. ä¸è¦è§£é‡Šæˆ–æ‰©å±•ä½ çš„å›ç­”ã€‚'",        height=150    )        # åº”ç”¨ç³»ç»Ÿæç¤ºè¯    if st.button("åº”ç”¨æç¤ºè¯"):        st.session_state["system_prompt"] = system_prompt        # æ›´æ–°å½“å‰å¯¹è¯çš„ç³»ç»Ÿæç¤ºè¯        if st.session_state["messages"] and st.session_state["messages"][0]["role"] == "system":            st.session_state["messages"][0]["content"] = system_prompt        else:            st.session_state["messages"].insert(0, {"role": "system", "content": system_prompt})        st.rerun()        st.header("å‚æ•°è®¾ç½®")        def update_temperature():        st.session_state["temperature"] = st.session_state.temperature_slider        def update_max_tokens():        st.session_state["max_tokens"] = st.session_state.max_tokens_slider        def update_top_p():        st.session_state["top_p"] = st.session_state.top_p_slider        def update_top_k():        st.session_state["top_k"] = st.session_state.top_k_slider        # æ¸©åº¦æ»‘å—    st.slider(        "æ¸©åº¦ (Temperature)",        min_value=0.1,        max_value=1.0,        value=st.session_state["temperature"],        step=0.1,        help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚è¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿è¾“å‡ºæ›´éšæœºã€‚",        key="temperature_slider",        on_change=update_temperature    )        # æœ€å¤§ç”Ÿæˆé•¿åº¦æ»‘å—    st.slider(        "æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max Tokens)",        min_value=64,        max_value=512,        value=st.session_state["max_tokens"],        step=64,        help="æ§åˆ¶ç”Ÿæˆå›å¤çš„æœ€å¤§é•¿åº¦ã€‚",        key="max_tokens_slider",        on_change=update_max_tokens    )        # Top-p æ»‘å—    st.slider(        "Top-p",        min_value=0.1,        max_value=1.0,        value=st.session_state["top_p"],        step=0.05,        help="æ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§ã€‚è¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´é›†ä¸­ï¼Œè¾ƒé«˜çš„å€¼ä½¿è¾“å‡ºæ›´å¤šæ ·ã€‚",        key="top_p_slider",        on_change=update_top_p    )        # Top-k æ»‘å—    st.slider(        "Top-k",        min_value=1,        max_value=100,        value=st.session_state["top_k"],        step=1,        help="æ§åˆ¶æ¯æ¬¡ç”Ÿæˆæ—¶è€ƒè™‘çš„å€™é€‰è¯æ•°é‡ã€‚",        key="top_k_slider",        on_change=update_top_k    )        st.header("èŠå¤©ç®¡ç†")        # æ–°å»ºèŠå¤©æŒ‰é’®    if st.button("æ–°å»ºèŠå¤©"):        chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")        st.session_state["current_chat"] = chat_id        st.session_state["messages"] = [            {"role": "system", "content": st.session_state["system_prompt"]}        ]        st.rerun()        # æ˜¾ç¤ºèŠå¤©å†å²åˆ—è¡¨    st.subheader("å†å²èŠå¤©")    chats = get_chat_list()    for chat in chats:        col1, col2 = st.columns([3, 1])        with col1:            if st.button(f"ğŸ“ {chat['date']}", key=f"chat_{chat['id']}"):                st.session_state["current_chat"] = chat["id"]                loaded_messages = load_chat_history(chat["id"])                if loaded_messages:                    st.session_state["messages"] = loaded_messages                st.rerun()        with col2:            if st.button("ğŸ—‘ï¸", key=f"delete_{chat['id']}"):                file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat['id']}.json")                if os.path.exists(file_path):                    os.remove(file_path)                if st.session_state["current_chat"] == chat["id"]:                    st.session_state["current_chat"] = None                    st.session_state["messages"] = [                        {"role": "system", "content": st.session_state["system_prompt"]}                    ]                st.rerun() # ==================== æ¨¡å‹åŠ è½½å’Œæ¨ç† ==================== @st.cache_resource def load_model(model_path):    """åŠ è½½æ¨¡å‹å’Œtokenizer        Args:        model_path: æ¨¡å‹è·¯å¾„        Returns:        tokenizerå’Œmodelçš„å…ƒç»„    """    tokenizer = AutoTokenizer.from_pretrained(model_path)    model = AutoModelForCausalLM.from_pretrained(model_path)    return tokenizer, model # åŠ è½½æ¨¡å‹ try:    model_path = AVAILABLE_MODELS[st.session_state["current_model"]]["path"]    tokenizer, model = load_model(model_path) except Exception as e:    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")    st.stop() # ==================== èŠå¤©ç•Œé¢ ==================== # å±•ç¤ºå†å²æ¶ˆæ¯ for msg in st.session_state.messages:    if msg["role"] != "system":        st.chat_message(msg["role"]).write(msg["content"]) # ç”¨æˆ·è¾“å…¥ if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯    st.session_state.messages.append({"role": "user", "content": prompt})    st.chat_message("user").write(prompt)     # æ„å»ºå¯¹è¯æ¨¡æ¿    messages = [{"role": "system", "content": st.session_state["system_prompt"]}]    messages.extend([msg for msg in st.session_state["messages"] if msg["role"] != "system"])        prompt_text = tokenizer.apply_chat_template(        messages, tokenize=False, add_generation_prompt=True    )    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)     # ç”Ÿæˆå›å¤    with st.chat_message("assistant"):        with st.spinner("æ€è€ƒä¸­..."):            outputs = model.generate(                **inputs,                max_new_tokens=st.session_state["max_tokens"],                do_sample=True,                temperature=st.session_state["temperature"],                top_k=st.session_state["top_k"],                top_p=st.session_state["top_p"]            )            response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)            st.write(response)            st.session_state["messages"].append({"role": "assistant", "content": response})                        # ä¿å­˜èŠå¤©è®°å½•            if st.session_state["current_chat"]:                save_chat_history(st.session_state["current_chat"], st.session_state["messages"])            else:                chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")                st.session_state["current_chat"] = chat_id                save_chat_history(chat_id, st.session_state["messages"]) 



###### ChatGPT said:

Updated saved memory

ä½ çš„ä»£ç æ•´ä½“ç»“æ„æ˜¯éå¸¸å®Œæ•´ä¸”æ¸…æ™°çš„ï¼Œæ ¸å¿ƒæ¨¡å‹å‚æ•°ï¼ˆå¦‚ temperatureã€top_pã€top_kã€max_tokensï¼‰éƒ½å·²ç»è®¾ç½®å¹¶æ­£ç¡®ä¼ é€’ç»™äº† `model.generate`ã€‚ä½†å¦‚æœä½ é‡åˆ°â€œé™¤äº† max_tokensã€ç³»ç»Ÿæç¤ºè¯å’Œæ¨¡å‹é€‰æ‹©ä»¥å¤–çš„å‚æ•°è°ƒæ•´æ— æ•ˆâ€çš„é—®é¢˜ï¼Œå¯èƒ½ä¸ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æœ‰å…³ï¼š

------

### âœ… å‚æ•°è®¾ç½®æ£€æŸ¥ä¸å»ºè®®

| å‚æ•°          | å½“å‰è®¾ç½®é€»è¾‘                              | æ˜¯å¦æ­£ç¡®ç”Ÿæ•ˆ | è¯´æ˜                                 |
| ------------- | ----------------------------------------- | ------------ | ------------------------------------ |
| `temperature` | é€šè¿‡ slider è®¾ç½®ï¼Œå¹¶ä¼ ç»™ `model.generate` | âœ…            | ç”Ÿæ•ˆæ— è¯¯                             |
| `top_p`       | é€šè¿‡ slider è®¾ç½®ï¼Œå¹¶ä¼ ç»™ `model.generate` | âœ…            | ç”Ÿæ•ˆæ— è¯¯                             |
| `top_k`       | åŒä¸Š                                      | âœ…            | ç”Ÿæ•ˆæ— è¯¯                             |
| `max_tokens`  | ä¼ é€’ä¸º `max_new_tokens` å‚æ•°              | âœ…            | ç”Ÿæ•ˆæ— è¯¯ï¼ˆæ³¨æ„ä¸ `max_length` åŒºåˆ†ï¼‰ |



------

### âš ï¸ å¯èƒ½çš„é—®é¢˜åŠè§£å†³å»ºè®®

#### 1. **éƒ¨åˆ†æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯å°æ¨¡å‹ï¼‰å¯¹å‚æ•°ä¸æ•æ„Ÿ**

- TinyLlamaã€Phi-2 ç­‰è¾ƒå°å‚æ•°é‡æ¨¡å‹åœ¨ sampling æ—¶å¯èƒ½è¡¨ç°å¾—â€œä¿å®ˆâ€ï¼Œå³ä¾¿ä½ è°ƒé«˜ `temperature` å’Œ `top_p`ï¼Œç”Ÿæˆç»“æœä¹Ÿå¯èƒ½æ¯”è¾ƒç¨³å®šã€‚
- **å»ºè®®**ï¼šä½ å¯ä»¥å°è¯•å°† `temperature=1.0`ã€`top_p=1.0`ã€`top_k=100` ç»„åˆæ¥æµ‹è¯•é‡‡æ ·å¤šæ ·æ€§æ˜¯å¦æ˜æ˜¾å¢å¼ºã€‚

#### 2. **ç³»ç»Ÿæç¤ºè¯æœªèƒ½å½±å“è¾“å‡º**

ä½ å·²æ­£ç¡®å°†ç³»ç»Ÿæç¤ºè¯è®¾ä¸ºç¬¬ä¸€æ¡ system æ¶ˆæ¯å¹¶åŠ å…¥æ¨¡æ¿ä¸­ã€‚ä½†ä¸åŒæ¨¡å‹è§£æ prompt çš„èƒ½åŠ›ä¸åŒï¼š

- âœ… **Qwen å’Œ OpenChatï¼ˆæ”¯æŒ ChatMLï¼‰**ï¼š`tokenizer.apply_chat_template` æœ‰è¾ƒå¥½æ•ˆæœã€‚

- âš ï¸ **Phi-2ã€TinyLlama**ï¼šéƒ¨åˆ†æ¨¡å‹å¹¶ä¸ä½¿ç”¨ Chat æ¨¡å¼ç»“æ„ï¼ˆå¯èƒ½æ—  `system` role çš„å¤„ç†é€»è¾‘ï¼‰ï¼Œæˆ–è€…åªå­¦ä¹ äº†ç®€å•çš„å¯¹è¯é£æ ¼ã€‚

- **è§£å†³å»ºè®®**ï¼š

  - æ›¿æ¢ `tokenizer.apply_chat_template(...)` ä¸ºä½ è‡ªå·±æ‹¼æ¥ prompt çš„æ–¹å¼ï¼š

    ```
    python
    
    
    CopyEdit
    prompt_text = st.session_state["system_prompt"] + "\nUser: " + prompt + "\nAssistant:"
    ```

  - æˆ–æ£€æŸ¥å¯¹åº”æ¨¡å‹ `config.json` ä¸­æ˜¯å¦åŒ…å« `"chat_template"` å­—æ®µï¼Œæ²¡æœ‰çš„è¯éœ€æ‰‹åŠ¨æ„é€ ã€‚

#### 3. **å‚æ•°å˜æ›´æœªè§¦å‘æ¨¡å‹é‡æ–°ç”Ÿæˆ**

- å¦‚æœä½ ä»…ä¿®æ”¹äº† slider æ²¡ç‚¹â€œå‘é€æ¶ˆæ¯â€ï¼Œä¸ä¼šè§¦å‘ `generate()`ï¼Œæ‰€ä»¥çœ‹èµ·æ¥â€œæ— æ•ˆâ€ã€‚
- **å»ºè®®**ï¼šåœ¨ç•Œé¢ä¸ŠåŠ ä¸ªâ€œé‡æ–°ç”Ÿæˆä¸Šä¸€æ¡å›å¤â€æŒ‰é’®ï¼Œæ–¹ä¾¿æµ‹è¯•å‚æ•°æ•ˆæœã€‚

------

### âœ… ä½ çš„è®¾ç½®ä»£ç çš„æ­£ç¡®æ€§æ€»ç»“ï¼š

| é¡¹ç›®                       | çŠ¶æ€        | å»ºè®®                                   |
| -------------------------- | ----------- | -------------------------------------- |
| æ¨¡å‹å‚æ•°è®¾ç½®               | âœ…           | æ— éœ€ä¿®æ”¹                               |
| ç³»ç»Ÿæç¤ºæ³¨å…¥ä½ç½®           | âœ…           | å¦‚æœæ¨¡å‹æ”¯æŒ chat æ¨¡å¼æ˜¯å¯ä»¥çš„         |
| `apply_chat_template` ä½¿ç”¨ | âœ…           | ä½†è¦ç¡®è®¤æ¨¡å‹æ˜¯å¦çœŸçš„æ”¯æŒ Chat æ¨¡æ¿     |
| èŠå¤©è®°å½•ä¿å­˜               | âœ…           | åŠŸèƒ½å®Œå¤‡                               |
| å‚æ•°å˜åŠ¨ç”Ÿæ•ˆ               | âœ…ï¼ˆé€»è¾‘ä¸Šï¼‰ | ä½†éœ€ç¡®ä¿è§¦å‘äº† `generate` æ‰ä¼šçœ‹åˆ°æ•ˆæœ |



------

å¦‚æœä½ å¸Œæœ›æˆ‘å¸®ä½ è¿›ä¸€æ­¥æ£€æŸ¥æŸä¸ªå…·ä½“æ¨¡å‹æ˜¯å¦çœŸæ­£æ”¯æŒ `system` promptã€`chat_template` æˆ–é‡‡æ ·å‚æ•°ï¼Œä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ å½“å‰ä½¿ç”¨çš„æ˜¯å“ªä¸€ä¸ªæ¨¡å‹ï¼ˆæˆ–è´´å‡ºè¯¥æ¨¡å‹åœ¨ Hugging Face ä¸Šçš„é“¾æ¥ï¼‰ã€‚æˆ‘å¯ä»¥ä»å…¶é…ç½®æˆ–æ–‡æ¡£å¸®ä½ ç¡®è®¤ã€‚